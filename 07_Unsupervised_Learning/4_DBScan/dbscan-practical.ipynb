{"metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# DBSCAN\n", "\n", "For this practical we will be considering a dataset containing the credit card usage behavior of about 9000 active credit card holders during a 6 month period. \n", "\n", "We will use DBSCAN to cluster the activity and see if it corresponds to groups of spending types. \n", "\n", "\n", "### The data dictionary\n", "\n", "The data contains 18 variables being the following:\n", "\n", "| Variable                       | Description                                                                                                                 |\n", "|--------------------------------|-----------------------------------------------------------------------------------------------------------------------------|\n", "| CUSTID                         | Identification of Credit Card holder (Categorical)                                                                          |\n", "| BALANCE                        | Balance amount left in their account to make purchases                                                                      |\n", "| BALANCEFREQUENCY               | How frequently the balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)           |\n", "| PURCHASES                      | Amount of purchases made from account                                                                                       |\n", "| ONEOFFPURCHASES                | Maximum purchase amount done in one-go                                                                                      |\n", "| INSTALLMENTSPURCHASES          | Amount of purchase done in installment                                                                                      |\n", "| CASHADVANCE                    | Cash in advance given by the user                                                                                           |\n", "| PURCHASESFREQUENCY             | How frequently the purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased) |\n", "| ONEOFFPURCHASESFREQUENCY       | How frequently purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)                   |\n", "| PURCHASESINSTALLMENTSFREQUENCY | How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)                      |\n", "| CASHADVANCEFREQUENCY           | How frequently the cash in advance is being paid                                                                            |\n", "| CASHADVANCETRX                 | Number of transactions made with \"Cash in Advance\"                                                                          |\n", "| PURCHASESTRX                   | Number of purchase transcations made                                                                                        |\n", "| CREDITLIMIT                    | Limit of credit card for user                                                                                               |\n", "| PAYMENTS                       | Amount of payments made by the user                                                                                         |\n", "| MINIMUM_PAYMENTS               | Minumum amount of the payments made by the user                                                                             |\n", "| PRCFULLPAYMENT                 | Percentage of full payments which has been paid by the user                                                                 |\n", "| TENURE                         | Tenure of credit card service for the user                                                                                  |\n", "\n", "Source is [here](https://www.kaggle.com/arjunbhasin2013/ccdata)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Import the necessary libraries\n", "\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "import seaborn as sns\n", "from sklearn.decomposition import PCA\n", "from sklearn.cluster import DBSCAN\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.metrics.pairwise import pairwise_distances\n", "\n", "# Turn off pink warnings\n", "\n", "import warnings\n", "warnings.filterwarnings('ignore')"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## 1) Load and transform the data\n", "\n", "\n", "Read the data contained in `data/CC GENERAL.csv` into a pandas dataframe called `df`, whilst also dropping the column called `CUST_ID`. Then take a look at the data (via the `.head`) method to check the formatting."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Check if there are null values in the data using the function `isnull()`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We find that there are some missing values but it is a very small proportion of the dataset and only present in 2 columns. Therefore, drop the rows with missing values and store this in `df`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Handling the missing values \n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can now check the data type for each column and confirm that it is as expected."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Check the data types\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["##\u00a02) Explore the data\n", "\n", "Using the `describe` function, explore the statistical properties of the dataset. By considering the 75th percentile and the maximum values, what can we say about certain columns of the data?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["There are large outliers in the columns: `BALANCE`, `PURCHASES`, `ONEOFF_PURCHASES`, `INSTALLMENTS_PURCHASES`, `CASH_ADVANCE`, `CREDIT_LIMIT`, `PAYMENTS` and `MINIMUM_PAYMENTS`. We can see this by approximately considering if the maximum or minimum values are more than the mean plus/minus twice the standard deviation.\n", "\n", "Therefore we will fit the DBSCAN model to the full dataset and scale our data to handle the skew. As an extension, we later consider a simple approach to handle this by dividing the data into categories. \n", "\n", "Extension: [This](https://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561) is a good article on common outlier detection techniques. The z-score provides an easy way to quickly identify possible outliers, however note that it does assume a normal distribution for your data."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## 3) Scaling the data for clustering"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Remember that when considering the distance between points it is important to scale the data. This is so that we don't increase the importance of variables which have larger values by nature. \n", "\n", "For this dataset we will rescale the data using the following steps:\n", "\n", "1) Using `StandardScaler`, rescale the data and save it in a variable called `scaled_data`.\n", "    Hint: For this define a `StandardScaler` object and use it with the `fit_transform` function on our dataframe. Store the scaled data as `scaled_data`.\n", "  \n", "2) Convert `scaled_data` to a pandas dataframe with the same columns as `df`. Store this as `scaled_df`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# 1) Scale the data to bring all the attributes to a comparable level \n", "\n", "# 2) Converting to a pandas DataFrame \n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let us quickly check that the `StandardScaler` has worked by using `describe` to check the new means and standard deviations. Our scaled data should now have a mean of zero (or as close as a machine can get!) and a standard deviation of 1. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["scaled_df.describe()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## 4) Investigate a starting value for eps\n", "\n", "Measure the distance of each point to its closest neighbor. To do this, use the function `sklearn.metrics.pairwise.pairwise_distances` on the scaled dataset, `scaled_df`. Let the output of `pairwise_distances` be stored as `all_distances`, and store the list of neighbour distances as `neig_distances`.\n", "\n", "Note that the documentation is given [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Plot the distribution of the distances using the function `hist`. Use enough bins to clearly see the behaviour at the tail, around 100-200 should be sufficient. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Plot the distances\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Looking at your graph what distances look appropriate for eps? Remember we want to choose a value slightly higher than the optimal one and then revise it down, so we'll choose an intial value to the right of the distribution. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## 5) Principal Component Analysis (PCA) of data\n", "\n", "Initially we will just use these two principal components to easily plot the data, however later in the exercise we will also explore how we can apply DBSCAN to them directly. \n", "\n", "Get the first 2 principal components of the data. To do this, use the function `PCA` to create a `pca` object and then use the `fit_transform` function to apply it to the scaled data. Store the result in a dataframe called `X_principal` with columns `P1` and `P2`. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## 6) Fit the DBSCAN model \n", "\n", "Apply DBSCAN, starting with `eps=2` and `min_samples=34` using the functions `DBSCAN` and `fit_predict`, storing the output as `cluster_assignment`. Then print out the unique clusters found. Note that the cluster labelled as `-1` are identified as noise/outliers by the DBSCAN model.\n", "\n", "Remember to apply the function to `scaled_df`. You can be explicit in specifying the columns of interest by using `.iloc[0:17]`. This is so that if adding columns containing the cluster assignments to our dataframe, we don't include them in any modelling later on.\n", "\n", "Note: We start with `eps=2` as this sits toward the right tail of our histogram of distances so this is a suitable first choice. We start with `min_samples=34` as a good rule of thumb is to start with twice the number of features, here ` 2 x 17`, this can then be increased if needed. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Assign a column called `labels` to `scaled_df` containing the cluster assignment"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Create a scatter plot with `x=X_principal['P1']` and `y=X_principal['P2']` and set the colour parameter `c=cluster_assignment`. Also change the size of the points using `s=3`, this will make it clearer to see the dense regions."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can see that our model has captured one cluster and termed the remaining points as noise. These two groups seem to overlap when we visualise them using the first two principle components, why do you think this is?"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can now explore the distribution of each variable for each of the clusters identified. Use the below code to explore these properties and see what conclusions we can make about each cluster. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["for c in scaled_df.iloc[:,0:17]:\n", "    grid= sns.FacetGrid(scaled_df, col='labels')\n", "    grid.map(plt.hist, c)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["It looks like the non-noise cluster corresponds to a specific type of customer behaviour. This could be used in several key ways including seeing if an anomalous payment was made by a customer based on their behaviour cluster. Alternatively on a business level, different marketing strategies can be pitched to each group in order to maximise results. \n", "\n", "Specifically for this clustering outcome we find that: \n", "\n", "Cluster 0: Customers with a typically low one-off purchase frequency and consistently high tenure. \n", "\n", "Note: this clustering was produced using our initial guesses of `eps=2` and `min_samples=34`. As we've done nothing to tune these values they're likely not optimal and probably don't accurately represent the underlying patterns or groups within the data."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## 7) Additional modelling\n", "\n", "We can also consider running DBSCAN on the PCA components. To do this run the same DBSCAN as previously but on the dataset `X_principal` using the same values for `eps` and `min_samples` and store the output once again in `cluster_assignment`.\n", "\n", "Then print out the unique clusters"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Plot the results with the new cluster assignment. As before, create a scatter plot of the principal components, coloured by the new cluster assignment."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Then plot the distribution of the results to identify the interpretation of each cluster in terms of the credit card user's behaviour. To do this: \n", "\n", "1) Assign the new cluster assignment to a column of `scaled_df` called `pca_labels`. \n", "\n", "2) Then copy the code used previously for the distributional plots, changing the `col` parameter to be the new `pca_labels`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["This time nearly all of the points have been assigned to the cluster with very few points being classed as outliers. Even though our model hyperparameters have remained the same our output has changed, this is because the modelling was perfomed on the 2 dimensional PCA data space which is denser. Why do you think the PCA space is denser than the original space? How might we change our `eps` and `min_values` given that we've moved to a denser space?"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Extension: Handling skewed data\n", "\n", "As noticed previously, there are large outliers in the columns: `BALANCE`, `PURCHASES`, `ONEOFF_PURCHASES`, `INSTALLMENTS_PURCHASES`, `CASH_ADVANCE`, `CREDIT_LIMIT`, `PAYMENTS` and `MINIMUM_PAYMENTS`.\n", "\n", "We are interested in finding similarities through clusters so it is helpful to group values in these columns with large outliers to categories. We'll then replace the category labels with the median value of each category, this allows us to preserve some information while (hopefully!) removing skew. \n", "\n", "Visualise this by first plotting the distribution of `df['PURCHASES']` using the `hist` function."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Print the maximum value in the column `Purchases`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["As we can see, there is a very large spread of values. Therefore first create a copy of `df` and name it `categorical_df`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Run the below code to convert the columns listed to categories. The new columns will be stored in columns with the same name, but with `_MEDIAN` appended. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["columns=['BALANCE', 'PURCHASES', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'CREDIT_LIMIT',\n", "                'PAYMENTS', 'MINIMUM_PAYMENTS']\n", "\n", "categorical_df = df.copy()\n", "\n", "for c in columns:\n", "\n", "    Range=c+'_MEDIAN'\n", "    categorical_df[Range]=0        \n", "    categorical_df.loc[((categorical_df[c]>0)&(categorical_df[c]<=500)),Range]=1\n", "    categorical_df.loc[((categorical_df[c]>500)&(categorical_df[c]<=1000)),Range]=2\n", "    categorical_df.loc[((categorical_df[c]>1000)&(categorical_df[c]<=3000)),Range]=3\n", "    categorical_df.loc[((categorical_df[c]>3000)&(categorical_df[c]<=5000)),Range]=4\n", "    categorical_df.loc[((categorical_df[c]>5000)&(categorical_df[c]<=10000)),Range]=5\n", "    categorical_df.loc[((categorical_df[c]>10000)),Range]=6"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Run the below code to replace each category with the median value from the original column.\n", "\n", "Note: `c[:-7]]` removes the \"_MEDIAN\" from our column names allowing us to access the original column."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["for c in categorical_df.columns[-len(columns):]:\n", "    for i in range(1, 7):\n", "        col_median = categorical_df.loc[categorical_df[c]==i, c[:-7]].median()\n", "        categorical_df.loc[categorical_df[c]==i, c] = col_median\n", "\n", "categorical_df.head()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can check how the summary statistics have changed by using `describe` on the transformed data and original data."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["categorical_df.iloc[:,-len(columns):].describe()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["categorical_df[columns].describe()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["As expected the max values for each column have been reduced while the means have remained mostly the same. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Then drop the columns given by the list `columns` from the dataframe `categorical_df`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Check that we have the same number of features in our transformed dataframe as in our original by comparing the `shape`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["As previously, we can now scale the data contained in `categorical_df`. Store the result in `scaled_cat_df`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# 1) Scale the data to bring all the attributes to a comparable level \n", "\n", "# 2) Converting to a pandas DataFrame \n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Compute the first two principal components of the data contained in `scaled_cat_df` and save this in a dataframe called `X_cat_principal`. You can see the first rows using the `head` function. We'll use this to plot our new clusters."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Find a starting value for eps by using the distribution of pairwise distances of the principal component dataframe `X_principal` and plot the results as a histogram. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Starting with `eps=1.1` and `min_samples=34`, run the DBSCAN algorithm on `scaled_cat_df` and print out the unique numbers of clusters."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Run `value_counts` on a Series containing the `cluster_assignment`. Does this look like a more promising clustering attempt?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Plot the result in a scatter plot of the principal components and coloured by the clustering assignment."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Finally use the below code to explore the results and interpretation of each cluster. You can toggle the `eps` and `min_samples` parameters to see how the clusters can vary both visually and distributionally. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["scaled_cat_df['pca_cat_labels'] = cluster_assignment\n", "for c in scaled_cat_df.iloc[:,0:17]:\n", "    grid= sns.FacetGrid(scaled_cat_df, col='pca_cat_labels')\n", "    grid.map(plt.hist, c)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["This is just one way of dealing with skewed data when performing cluster analysis. For example another approach would be to take a log transform of our data before scaling it. "]}]}
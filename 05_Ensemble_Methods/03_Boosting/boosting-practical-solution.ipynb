{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}, "vscode": {"interpreter": {"hash": "5739bf39c17db21ae60b8ce29dba7a6315d7ad17310a9a52cb1da7909ba2b1a6"}}}, "nbformat": 4, "nbformat_minor": 4, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Boosting with LightGBM\n", "\n", "In this practical we will use the `lightgbm` package to perform classification.\n", "\n", "We'll use a dataset of online behaviour, with the aim of classifying visitors as likely to make, or not make, a purchase."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Classification: the data\n", "\n", "The dataset is from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Online+Shoppers+Purchasing+Intention+Dataset).\n", "\n", "Each row summarises a visitor's history on an e-commerce site (12,330 visits in total).\n", "\n", "The features are related to the type of page visited (and duration), various analytics metrics (bounce/exit rates), OS/browser used, region, and so on.\n", "\n", "The binary variable we want to predict is \"Revenue\" - did the user eventually make a purchase?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import pandas as pd\n", "\n", "data = pd.read_csv('data/online_shoppers_intention.csv')\n", "\n", "data.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["data.info()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Preparing the data - I\n", "\n", "`lightgbm` requires data to be `int`, `float`, or `bool`. Categorical items will also need to be identified as such.\n", "\n", "From looking at the dataset above, we have two categorical columns which are `str` type. Some other columns are `int` but represent categorical data too, e.g. `Region`.\n", "\n", "Use the `.astype('category')` method available for `pandas` Series (each column in a DataFrame is a Series) to change these columns to the correct datatype."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "\n", "data[['Month', 'VisitorType', 'OperatingSystems', 'Browser', 'Region', 'TrafficType']] = data[['Month', 'VisitorType', 'OperatingSystems', 'Browser', 'Region', 'TrafficType']].astype('category')\n", "\n", "data.info()\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Preparing the data - II\n", "\n", "As is standard, we will split up the data using `sklearn.model_selection.train_test_split` to create a training set and a test set. We will keep the test set aside to evaluate the models on unseen data, as a measure of how generalisable they are.\n", "\n", "First, separate the predictive columns (the `X`) from the outcome variable (the `y`).\n", "\n", "Then create your train and test sets using the default ratio of `train_test_split` and set the random state to be `136`. As we have imbalanced data set `stratify=y`, this will ensure that we have the same `y` ratio in both the train and test sets.\n", "\n", "Call the variables `X_train`, `X_test`, `y_train`, and `y_test`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "# Your code here...\n", "\n", "X = data.drop('Revenue', axis=1)\n", "y = data.Revenue\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=136)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Run the following cells to confirm that the target ratio is the same in both the train and test sets."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# y_train.value_counts(normalize=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# y_test.value_counts(normalize=True)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Preparing the data - III\n", "\n", "Normally we need to pre-process data for `sklearn` models. `lightgbm` comes with a `Dataset` class that will handle the data for us. It will use the column names of the `pandas` DataFrame to name the features.\n", "\n", "For more advanced use, it can also apply individual weights to specific items in the data.\n", "\n", "Set up two instances of `lgb.Dataset()` named `lgb_train` and `lgb_test`.\n", "\n", "You can do this by passing the appropriate `X_` and `y_` arrays you created above.\n", "\n", "For `lgb_test`, set the argument `reference=lgb_train`. This is so that when the model is pre-processing data (i.e. discretizing continuous features into histogram bins, and trying to combine categorical features,) it has access to the full range of possible data values, not just the ones that appear in the testing data.\n", "\n", "For more information on what `lgb.Dataset()` is doing see the [documentation](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Dataset.html)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import lightgbm as lgb\n", "\n", "# Your code here...\n", "\n", "lgb_train = lgb.Dataset(X_train, y_train)\n", "\n", "lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Setting parameters\n", "\n", "`lightgbm` is highly configurable - see [the documentation](https://lightgbm.readthedocs.io/en/latest/Parameters.html) for a full list.\n", "\n", "Some parameters are related to:\n", "\n", "- performance: Using the GPU or multiple machines to speed up training\n", "- task: regression, classification (binary or multi), ranking\n", "- boosting algorithm: gradient boosting decision trees? Random forests? Something else?\n", "- ensemble model parameters: learning rate, number of models in ensemble\n", "- model-specific parameters: depth of decision trees, number of nodes per level\n", "\n", "For now we will focus on parameters which are related to:\n", "\n", "* our binary classification task (the objective and metric used)\n", "* how boosting is done (the learning rate for gradient descent)\n", "* decision trees (number of leaves, depth)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["params = {\n", "    'boosting_type': 'gbdt',\n", "    'objective': 'binary',\n", "    'metric': 'binary_logloss',\n", "    'num_leaves': 31,\n", "    'learning_rate': 0.1,\n", "    'max_depth': -1,\n", "    'verbose': 0,\n", "    'seed': 0,\n", "}"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["* `max_depth: -1` means no limit in `max_depth`\n", "* `verbose: 0` only displays warnings\n", "* `seed: 0` ensures we get the same results"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Training the model - I\n", "\n", "This is done by calling the `lgb.train()` function and passing it the parameters, the training set and a validation set.\n", "\n", "We will validate on the training set. By default it will run for 100 iterations of boosting.\n", "\n", "In order to capture and plot the logloss for each boosting iteration we need to create an empty dictionary `evals={}` and set `callbacks=[lgb.record_evaluation(evals)]` within `lgb.train()`. We can then use the inbuilt `plot_metric()` function to show how the logloss changes across iterations.\n", "\n", "What do you observe as the model trains?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["evals={}\n", "\n", "model = lgb.train(params=params,\n", "                  train_set=lgb_train,\n", "                  valid_sets=[lgb_train],\n", "                  valid_names=['train'],\n", "                  callbacks=[lgb.record_evaluation(evals), \n", "                             lgb.log_evaluation(10)]\n", "                  \n", "                 )\n", "\n", "lgb.plot_metric(evals)\n", "\n", "# Your thoughts below...\n", "print('='*100)\n", "print(\"The logloss decreases over the 100 iterations by around 70%. We still don't know how the model performs in terms of the actual classification task though!\")\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Training the model - II\n", "\n", "`lightgbm` makes it easy to test on the unseen data at each iteration. Just add the `lgb_test` object to `valid_sets`.\n", "\n", "Remember to set up an `evals={}` and include the `callback` argument, `lightgbm` will record and plot the logloss for all the specified sets.\n", "\n", "How does the loss on the unseen data compare?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "\n", "evals={}\n", "\n", "model = lgb.train(params=params,\n", "                  train_set=lgb_train,\n", "                  valid_sets=[lgb_train, lgb_test],\n", "                  valid_names=['seen', 'unseen'],\n", "                  callbacks=[lgb.record_evaluation(evals),\n", "                             lgb.log_evaluation(10)]\n", "                 )\n", "\n", "\n", "lgb.plot_metric(evals)\n", "\n", "# Your thoughts below...\n", "print('='*100)\n", "print(\"The logloss for the unseen set reaches a minimum after 20 interations and increases after that. This implies that we're probably overfitting!\")\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Training the model - III\n", "\n", "Above we only used a measure of logloss to evaluate the model. In classification tasks, we normally want to know  other metrics like auc and precision. \n", "\n", "Update `params` so that `metric` is a list that also includes `\"auc\"` and `\"average_precision\"`.\n", "\n", "This time run the model for 200 iterations by changing the value of `num_boost_round` when calling `lgb.train()`. Remember to record the `evals` using the method from above but don't plot them just yet, we'll do that next!\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# A dictionary to store the training results...\n", "evals = {}\n", "\n", "# Your code here...\n", "\n", "params = {\n", "    'boosting_type': 'gbdt',\n", "    'objective': 'binary',\n", "    'metric': ['binary_logloss', 'auc', 'average_precision'],\n", "    'num_leaves': 31,\n", "    'learning_rate': 0.1,\n", "    'max_depth':-1,\n", "    'verbose': 0,\n", "    'seed': 0,\n", "}\n", "\n", "model = lgb.train(params=params,\n", "                  train_set=lgb_train,\n", "                  valid_sets=[lgb_train, lgb_test],\n", "                  valid_names=['seen', 'unseen'],\n", "                  num_boost_round=200,\n", "                  callbacks=[lgb.record_evaluation(evals),\n", "                             lgb.log_evaluation(10)]\n", "                 )\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Training the model - IV\n", "\n", "Run the cell below to plot the data you stored in `metrics`. It loops through the individual metrics and plots them at each iteration.\n", "\n", "What do you observe?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "\n", "f, a = plt.subplots(1,3, figsize=(16,5))\n", "\n", "for e, m in enumerate(['binary_logloss', 'auc', 'average_precision']):\n", "    lgb.plot_metric(evals, metric=m, ax=a[e], title=f\"{m} during training\")\n", "    \n", "    \n", "plt.tight_layout()\n", "# Your thoughts below...\n", "\n", "print(\"The model is overfitting quite quickly - 200 iterations is way too much. Peak unseen performance is likely around 25 iterations.\")\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Investigating feature performance\n", "\n", "`lightgbm` has built-in methods for seeing which features were most predictive.\n", "\n", "There are two ways of measuring feature importance: \n", "\n", "* `split`: how many times a feature is used to make a decision in all the trees in the ensemble, except for leaf nodes. A feature which is used often to make decisions along the way is likely very informative.\n", "* `gain`: the sum of the information gain score from using a feature in a tree.\n", "\n", "These will each be normalised by the number of trees in the ensemble.\n", "\n", "A trained `lightgbm` model has a very handy `.plot_importance()` method. Before running the cells below, what do you predict the most/least useful features will be when trying to predict whether a visitor to a website will make a purchase?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["for imp_type in ['split', 'gain']:\n", "    ax = lgb.plot_importance(model,\n", "                             importance_type=imp_type,\n", "                             dpi=100,\n", "                             title=f\"Feature importance by {imp_type}\"\n", "                             )\n", "# Your thoughts below...\n", "\n", "print(\"Unsurprisingly, features which relate to human activity (duration spent on pages etc.) are more informative than which operating system or browser is being used.\")\n", "print(\"The feature related to how close the date is to a special day of the year was not very useful, which may be a surprise if you thought people would buy more around holidays etc.\")\n", "print(\"This information could help you decide which kind of data to focus on gathering in the future.\")\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Getting predictions for new data\n", "\n", "So far we have used only the data available, but in practice it is likely that more data will come in and you will want to evaluate that.\n", "\n", "In `lightgbm` you can save/load trained models and continue training them, if you have more labeled data.\n", "\n", "If you want predictions for unlabelled data, you can do that too. You can pass a DataFrame of just the `X` values.\n", "\n", "The model's `.predict()` method will return a score for each item. If it is >= 0.5 then it is in the positive class, otherwise the negative.\n", "\n", "The two examples below would be classified as leading to a sale."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["new_data_points = [\n", "    [2.7, 99, 0.2, 30.0, 60, 12445.0, 0.0, 0.0, 20.5, 0.8, 'Feb', 1, 3, 2, 2, 'Returning_Visitor', True],\n", "    [1.2, 99, 0.2, 1.0, 20, 942.0, 0.0, 0.0, 1.5, 0.28, 'Mar', 1, 1, 1, 1, 'Returning_Visitor', False],\n", "]\n", "\n", "new_data_points = pd.DataFrame(new_data_points, columns=data.columns[0:-1])\n", "\n", "new_data_points[['Month', 'VisitorType', 'OperatingSystems', 'Browser', 'Region', 'TrafficType']] = new_data_points[['Month', 'VisitorType', 'OperatingSystems', 'Browser', 'Region', 'TrafficType']].astype('category')\n", "\n", "model.predict(new_data_points)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["As `lightgbm` models output probabilities we can write a function to convert them into class labels. We'll use the default probability threshold of 0.5 (but this can be changed!). We need to convert to class labels if we want to make use of tools like `sklearn.metrics.classification_report`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import numpy as np\n", "\n", "def predict_class(model, data, threshold=0.5):\n", "    \n", "    y_pred = model.predict(data)\n", "    \n", "    return np.where(y_pred > threshold, 1, 0)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's check `predict_class()` on the new data points defined above."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["predict_class(model, new_data_points)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Use `predict_class()` to calculate the `sklearn.metrics.accuracy_score` of our model on the test set."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import accuracy_score\n", "\n", "# Your code below...\n", "accuracy_score(y_test, predict_class(model, X_test))\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["If you also calculate the accuracy score for the training data you should get around 0.99, much higher than our test accuracy! We're clearly overfitting so let's do something about that."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Finding the best model hyperparameters\n", "\n", "As we saw, there are many hyperparameters to be tweaked - see [the documentation](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html) for more details.\n", "\n", "One method is to try each combination manually, but this is time-consuming and poor practice.\n", "\n", "Instead, `lightgbm` can be used with `sklearn`'s  cross-validation methods. It will train multiple models on subsets of the data (and evaluate on the unseen remainder) for each combination of parameters and find the best combination.\n", "\n", "The model we trained had around 90% accuracy on our test data but was doing much better on our training data. `lightgbm` makes the following suggestions for reducing overfitting:\n", "\n", "* Use small `max_bin` \n", "* Use small `num_leaves` \n", "* Increase `min_data_in_leaf`, this is the minimal number of data in one leaf\n", "* Use `min_gain_to_split` to regularize the model growth\n", "\n", "We can try a few of these and some others.\n", "\n", "The `GridSearchCV` class from `sklearn` takes a model, with a dictionary of hyperparameter and values. Then you just fit/train it as usual, using the training dataset we created at the beginning, `X_train` and `y_train`.\n", "\n", "We use `lgb.LGBMClassifier()` as our model here, we assign it to the variable `classifier`. Note that this is different to the syntax that we used above as weren't needing to communicate with `sklearn`.\n", "\n", "Below, create a `GridSearchCV` in the same way you would an `sklearn` model: assign it to a variable named `gcv`, pass it the `classifier` as your basic model without parameters set, and also pass it `params`.\n", "\n", "To speed things up, set `n_jobs=-1` to use all available CPU cores. Set `verbose=1` in the `GridSearchCV` so you get updates as it proceeds - useful for making sure it is actually working! Note that we've set `verbose=-1` in the model params so that it doesn't spit out 100s of warnings.\n", "\n", "This is 24 models across 5 folds (so 120 models trained in total), which should take approximately 1 minute on EDUKATE. If it is going too slowly for you, try trimming a few of the items from each of the params. Alternatively you can include a wider range of items by uncommenting some of the values below, this will take longer but will result in a higher accuracy. Remember you can interrupt and restart the kernel if it is taking too long."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.model_selection import GridSearchCV\n", "\n", "params = dict(\n", "    max_bin=[155, 255],#,355,455],\n", "    num_leaves=[7, 15, 31],#30, 40, 50],\n", "    min_data_in_leaf=[15, 20],#25],\n", "    min_gain_to_split=[0, 0.01],#0.02, 0.03],\n", "    verbose=[-1],\n", "    seed=[0],        \n", ")\n", "\n", "\n", "classifier = lgb.LGBMClassifier()\n", "\n", "# Your code below...\n", "\n", "gcv = GridSearchCV(estimator=classifier, param_grid=params, n_jobs=-1, verbose=1)\n", "\n", "gcv.fit(X_train, y_train)\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# What was the best model?\n", "\n", "`GridSearchCV` evaluated each possible model using the accuracy metric.\n", "\n", "The best model is stored inside `gcv` as `best_estimator_`. Its score is in `gcv.best_score_` and the actual hyperparameters used are in `gcv.best_params_`.\n", "\n", "(The score here is not the score on the training set, but the average score across subsets of the training set.)\n", "\n", "Take a look at these and then evaluate the best model on the test set `X_test` and `y_test` - the model in `gcv.best_estimator_` has a `.score()` method you can use. We don't need to use our `predict_class` function as we're using the `LGBMClassifier` which outputs classes in order to match the syntax of the `sklearn` library.\n", "\n", "You should also check the performance of the `gcv.best_estimator_` on the training data, if there's still a gap then we're likely to be overfitting.\n", "\n", "How does it compare to the model you trained before?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code and thoughts below...\n", "\n", "print(f\"Best model accuracy: {gcv.best_score_}\")\n", "\n", "print(f\"Best hyperparameters: {gcv.best_params_}\")\n", "\n", "best_model = gcv.best_estimator_\n", "\n", "print(f\"Accuracy on seen data: {best_model.score(X_train, y_train)}\")\n", "\n", "print(f\"Accuracy on unseen data: {best_model.score(X_test, y_test)}\")\n", "\n", "# print(\"Accuracy has gone up to just over 90.7%.\")\n", "# print(\"The accuracy on both the seen and unseen data is far closer, this indicates that we've reduced the degree of overfitting that was takking place.\")\n", "# print(\"As the accuracy on the seen data is still higher we could probably continue to tune this model.\")\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# A closer look at the best model - I\n", "\n", "The feature importances can be found in the `.feature_importances_` attribute of the model - check the `.best_model.importance_type` attribute to see if the model used `split` or `gain` for importances.\n", "\n", "The feature names are stored in `.feature_name_`.\n", "\n", "Create a DataFrame from the two lists (importances and names) and see which features are most important for this model.\n", "\n", "Have they changed with these new hyperparameters?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["best_model.importance_type"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code and thoughts below...\n", "\n", "f_imp = pd.DataFrame(dict(feature=best_model.feature_name_, importance=best_model.feature_importances_))\n", "\n", "f_imp = f_imp.sort_values('importance', ascending=False)\n", "\n", "f_imp\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# A closer look at the best model - II\n", "\n", "We used accuracy so far to evaluate performance, but it would be better to know the per-class performance.\n", "\n", "Get the best model's predictions for `X_test` using its `.predict()` method.\n", "\n", "Then, compare these to the true `y_test` classes using `sklearn.metrics.classification_report`.\n", "\n", "Also take a look at the false positves/negatives using `sklearn.metrics.confusion_matrix`.\n", "\n", "What do you observe?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import classification_report, confusion_matrix\n", "\n", "# Your code and thoughts below...\n", "\n", "best_model_pred = best_model.predict(X_test)\n", "\n", "print(classification_report(y_test, best_model_pred))\n", "\n", "print(confusion_matrix(y_test, best_model_pred))\n", "\n", "print('='*53)\n", "\n", "print(\"The classes are not very balanced - far more people do not buy than do buy something.\")\n", "print(\"Performance on the most common class is very strong, but we're interested in trying to model if someone buys something.\")\n", "print(\"If we wanted to try and address this we could use a more class-imbalance sensitive metric in our gridsearch.\")\n", "print(\"We have nearly half as many false negatives as true positives, our model is clearly not capturing something about our users purchase behaviour. \")\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Conclusion and next steps\n", "\n", "In this practical we went through the process of using `lightgbm` to classify customers, based on their behaviour on a website.\n", "\n", "We trained and evaluated a single model before fine-tuning the hyperparameters to increase performance by a large margin.\n", "\n", "We focused on classification but `lightgbm` also does regression. To take this practical further, you could try applying what you have learned to a regression dataset - see [the UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets.php) for possible datasets.\n", "\n", "Look into the `lgb.LGBMRegressor()` class, which is an `sklearn`-style class. Or use the `lgb.train()` method and change the `params` dictionary so that `objective` and `metric` are suitable for regression.\n", "\n", "Another thing to bear in mind is that `lightgbm` is very fast - there's very little downside to just playing around with it!"]}]}
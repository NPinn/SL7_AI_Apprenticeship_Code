{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Part 1: Convolutions"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["In this first part you will see what convolutions do and how they work. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Load the Python libraries\n", "\n", "Let us start by loading the necessary Python libraries and set a few parameters for the notebook. the `misc` element from `scipy` will allow us to do some elementary image manipulation. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["%matplotlib inline"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "# for elementary image loading\n", "import imageio as iio\n", "\n", "# specifies the default figure size for this notebook\n", "plt.rcParams['figure.figsize'] = (10, 10)\n", "\n", "# specifies the default color map\n", "plt.rcParams['image.cmap'] = 'gray'"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Import an image\n", "\n", "To explore how convolutions work, you will use the portrait of [Grace Hopper](https://en.wikipedia.org/wiki/Grace_Hopper).\n", "Use the `imread` function from `imageio` to load the image. ([imread documentation](https://imageio.readthedocs.io/en/stable/userapi.html?highlight=imread#imageio.imread).\n", "\n", "The image corresponds to a matrix of dimension H x W x C (height by width by channels). Channels are used for color images. Use mean over the final channels dimensions to convert to a grey-scale image of H x W (height by width), where each entry corresponds to a pixel value between 0 (black) and 255 (white).\n", "\n", "Use `imshow` from `plt` to display the image. \n", "Since it's a matrix, you can use standard numpy style indexing to only show a region.\n", "Show the first 200 lines and the first 600 columns."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Load the grace_hopper.jpg image from the data folder\n", "# convert to grayscale\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Show the image\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Show another figure with only the first 200 rows / 600 cols\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Alternatively, you can view the values of the pixels directly, for example select the first five rows and columns (**top left corner**) and show the corresponding matrix. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Print the pixel values of a region in the top left corner\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Define and apply a convolution function\n", "\n", "Now lets define a convolution function. First you must define a function which traverses the image to apply the convolution at every point and returns the result in a filtered image. Calculating the size of the filtered image along each dimension can be a little tricky, the formula is: \n", "\n", "                         (Size of the filtered image) = (input image size) - (filter size) + 1\n", "\n", "Let us start by implementing the `convolve` function. It takes as input an image and a filter matrix, and returns\n", "the output of applying the filter at each position in the image through a function `multiply_sum`. \n", "\n", "The function `multiply_sum` corresponds to the following operation for two matrices $A$ and $B$ of identical dimensions:\n", "\n", "$$\n", "\\text{ multiply\\_sum }(A, B) = \\sum_{i,j} A_{ij}B_{ij}\n", "$$\n", "\n", "i.e. you form a matrix $C$ with entries corresponding to the entry-wise products and you sum across $C$\n", "\n", "**Note**: the implentations here are computationally inefficient (and you will see that applying it on the image takes a second). In practice, when dealing with thousands of images, you really don't want sub-optimal operations which is why libraries like Tensorflow hide away *a lot* of optimisations to make such operations as quick as possible and leverage the hardware that is available to you (e.g.: GPU)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# add your code to define the multiply_sum function (check that it works on a simple example)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The function below defines the convolution operation, go through the code and make sure it makes sense to you what is happening (there is a bit of fiddling required to apply the operations at the right place and store the results appropriately)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Convolution function\n", "def convolve(image, filter_matrix):\n", "    \n", "    # get the dimension of the filter\n", "    filter_height = filter_matrix.shape[0]\n", "    filter_width  = filter_matrix.shape[1]\n", "    \n", "    # allocate an empty array for the filtered image using the formula\n", "    # this is the array we'll use to store the result of the convolution\n", "    filtered_image = np.ndarray(shape=(image.shape[0] - filter_height + 1, \n", "                                       image.shape[1] - filter_width + 1))\n", "    \n", "    # go through rows\n", "    for row in range(filtered_image.shape[0]):\n", "        # go through columns\n", "        for col in range(filtered_image.shape[1]):\n", "            # select a local patch of the image\n", "            patch = image[row:(row + filter_height), \n", "                          col:(col + filter_width)]\n", "\n", "            # apply the multiply_sum operation\n", "            ms = multiply_sum(patch, filter_matrix)\n", "            \n", "            # store it at the right location\n", "            filtered_image[row, col] = ms\n", "            \n", "    return filtered_image"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["An there you have it, a convolution operator! You can apply a filter onto an image and see the result. \n", "\n", "Define a filter matrix corresponding to\n", "\n", "$$\n", "\\left(\\begin{array}{ccc}\n", "    -1&-1&-1\\\\ \n", "    2&2&2\\\\ \n", "    -1&-1&-1\n", "\\end{array}\\right)\n", "$$\n", "\n", "apply it to GH's portrait and display the result with `plot_filter`: a function we provide below"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def plot_filter(filtered_image, cmap=None):\n", "    if cmap is None:\n", "        cmap = plt.get_cmap('bwr')  # a diverging cmap\n", "    # set vmin = vmax such that 0 is in the centre of the cmap\n", "    vmax = int(\n", "        max(\n", "            np.abs(np.max(filtered_image)),\n", "            np.abs(np.min(filtered_image))\n", "        )\n", "    )\n", "    vmin = -vmax\n", "    im = plt.imshow(filtered_image, vmax=vmax, vmin=vmin, cmap=cmap)\n", "    plt.colorbar(im, fraction=0.1)\n", "    plt.axis('off')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# First define the 3x3 filter \n", "\n", "# Then apply it to the image using the convolve function defined above\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Finally show the result using our plot_filter function\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Quiz: What did our filter do?\n", "\n", "1) By looking at the image, can you tell what kind of pattern the filter detected?\n", "\n", "2) How would you design a filter which detects vertical edges?\n", "\n", "3) What would the following filter do: ([Prewitt operator](https://en.wikipedia.org/wiki/Prewitt_operator))\n", "\n", "$$\n", "\\left(\\begin{array}{ccc}\n", "    1&1&1\\\\ \n", "    0&0&0\\\\ \n", "    -1&-1&-1\n", "\\end{array}\\right)\n", "$$\n", "\n", "\n", "how about its transpose? how about if you swap the first and last row?\n", "\n", "Try variations until you get an intuition for what these operators do."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Define the filter matrix\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Show the result of convolving the image with this new filter\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Repeat with the transpose of the filter\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Repeat with the filter but with its first and last rows swapped\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Convolutions with colour\n", "\n", "Very good! But what if we had a coloured image, how would we use that extra information to detect useful patterns? \n", "The idea is simple: on top of having a set weight for each pixel, we have a set weight for each colour channel within that pixel.\n", "Filters become stacks of kernels (usually 3 for the three channels: R, G, B). \n", "\n", "An example is the following kernel which detects region of the image that are mostly brown."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["brown_filter = np.array(\n", "      [[[ 0.13871045,  0.17157242,  0.12934428], # Red channel\n", "        [ 0.16168842,  0.20229845,  0.14835016],\n", "        [ 0.135694  ,  0.16206263,  0.11727387]],\n", "\n", "       [[ 0.04231958,  0.05471011,  0.03167877], # Green channel\n", "        [ 0.0462575 ,  0.06581022,  0.03104937],\n", "        [ 0.04185439,  0.04734124,  0.02087744]],\n", "\n", "       [[-0.15704881, -0.16666673, -0.16600266], # Blue channel\n", "        [-0.17439997, -0.17757156, -0.18760149],\n", "        [-0.15435153, -0.17037505, -0.17269668]]])\n", "\n", "print(brown_filter.shape)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The **first** dimension corresponds the three channels (R, G, B) (try `brown_filter[1, :, :]` for the filter values corresponding to the green channel). \n", "\n", "Looking at the values, you can see that the filter responds to regions that are red (positive values, reasonably large), a little bit to the green values (positive values, quite small), and not at all to regions that are blue (negative values). \n", "\n", "To see it in practice, use `imread` from `imageio` without `as_gray=True` to load the coloured image of Grace Hopper.\n", "\n", "Show the image and its dimensions"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Load and display the coloured image of Grace Hopper\n", "\n", "# Show the shape of the image\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Now we would like to apply the Brown filter and see the result. \n", "One thing needs to be done, it's a bit annoying but it happens *all the time* in CNNs (and other non-trivial NNs): you need to adjust dimensions. \n", "Currently, it is the **first** dimension of the brown filter that corresponds to the colour channels while you saw that it is the **last** dimension of GH's image that correspond to the channels. \n", "\n", "Therefore you need to re-arrange dimensions from \n", "\n", "```\n", "(0, 1, 2) -----> (1, 2, 0)\n", "```\n", "\n", "this can be done via the `transpose` method: `array.transpose((1, 2, 0))`.\n", "\n", "Adjust the brown filter and apply it. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Adjust the dimensions of the brown filter and apply it to the image of GH\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Quiz\n", "\n", "Can you design a filter which will detect the edge from the background (blue) to Grace Hopper\u2019s left shoulder (black).\n", "\n", "**Note**: it's good practice to have the weights in your filter sum to 0 and don't forget to re-arrange the dimensions."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# devise a left_shouler_filter and apply it\n"]}]}
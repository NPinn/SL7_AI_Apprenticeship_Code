{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 4, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Introduction to Graph Neural Networks\n", "\n", "In this practical, we will be focusing on understanding graph data structures, and how conventional machine learning and deep learning approaches work with graph data. The notebook consists of two parts. \n", "\n", "- Part 1: Graph Data and Handling Graph Data\n", "- Part 2: Machine Learning with Graphs"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Part 1: Graph Data and Handling Graph Data\n", "\n", "In this section, we are going to understand graph data structures and do machine learning with graph data. Graph data is very common in the world we live in, and graph data structures provide a powerful representation to embed this information in a machine-readable manner. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Building a Graph in Python\n", "\n", "There are off-the-shelf libraries available to represent graph-like data using a graph data structure in Python. One of the most mature and popular libraries used for this purpose is the `NetworkX` library. [NetworkX](https://networkx.org/) is a Python package for the creation, manipulation, and study; of the structure, dynamics, and functions of complex networks. It is a 3-clause, BSD licensed, open-source library, that can be used to create graph structures and obtain useful information from the created graphs. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Installation"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import networkx as nx"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Constructing Graphs\n", "\n", "One can instantiate graphs in `NetworkX` using the `nx.Graph` class. `NetworkX` supports both *directed* and *undirected* graphs. \n", "\n", "<img src=\"img/graph.jpg\" class=\"wide\">\n", "\n", "Consider the above graph. We can use `nx.Graph` to instantiate an undirected, unweighted graph; and use the `add_node` and `add_edge` methods to express this graph. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Instantiate the graph\n", "G = nx.Graph()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Add nodes A, B, C, and D\n", "G.add_node(\"A\")\n", "G.add_node(\"B\")\n", "G.add_node(\"C\")\n", "G.add_node(\"D\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Exercise\n", "\n", "Now let us add the *four* undirected, unweighted edges to the graph `G`. The `add_edge` method found in [networkX API](https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.add_edge.html) allows doing this. \n", "\n", "***Hint***: the `weight` parameter can be used to explicitly state that every edge should get a weight of `1.0`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Add the edges. \n", "# Edge weight is kept constant to 1.0 as this is a unweighted graph\n", "\n", "# Your code here...\n", "G.add_edge(\"A\", \"B\", weight=1)\n", "G.add_edge(\"A\", \"C\", weight=1)\n", "G.add_edge(\"A\", \"D\", weight=1)\n", "G.add_edge(\"B\", \"D\", weight=1)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Once the nodes and the edges have been expressed in the graph `G` using `NetworkX`, your graph is ready. You can view the expressed graph using the `nx.draw()` function.\n", "\n", "Use the `nx.draw()` function to display graph `G`. You can refer to the documentation found [here](https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw.html#networkx.drawing.nx_pylab.draw) to fully understand how the drawing functionality works. \n", "\n", "***Hint***: You can set the `with_labels` parameter to `True` to also display the node labels. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "nx.draw(G, with_labels=True)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Attributes of a Graph\n", "\n", "Now that the graph is available in `NetworkX`, we can use all the functionality within `NetworkX` to get relevant information about the graph. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["One can investigate the number of nodes and number of edges in the graph. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["print(f\"Number of nodes in graph G: {G.number_of_nodes()}\")\n", "print(f\"Number of edges in graph G: {G.number_of_edges()}\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["One can also easily obtain useful information about the graph that is required to generate an *Adjacency Matrix* $\\mathcal{A}$ and a *Degree Matrix* $\\mathcal{D}$.\n", "\n", "#### Exercise\n", "\n", "Use the attributes of graph `G` to find out the statistics required to create the:\n", "1. Adjacency Matrix (assign to the variable `G_Adj`)\n", "2. Degree Matrix (assign to the variable `G_Deg`)\n", "\n", "***Hint***: You may refer to the `Graph` object documentation found [here](https://networkx.org/documentation/stable/reference/classes/graph.html) to identify the attributes for the relevant statistics. \n", "\n", "The matrix itself does not need to be obtained here. Gathering the statistics required to compute the respective matrices is sufficient. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Find the adjacency matrix of graph G\n", "\n", "# Your code here...\n", "G_Adj = G.adj\n", "\n", "print(\"The adjacency information of graph G:\")\n", "print(G_Adj)\n", "print(\"\\n\")\n", "\n", "# Find the degree statistics of graph G\n", "\n", "# Your code here...\n", "G_Deg = G.degree\n", "\n", "print(\"The degree information of graph G:\")\n", "print(G_Deg)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## CORA Dataset\n", "\n", "The CORA dataset is a citation network dataset that is available publicly from the [University of California, Santa Cruz](https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz). The dataset contains a collection of Computer Science research papers where the paper attributes, labels, and the citation graph is presented. The full dataset contains 2708 research papers (nodes in the graph) and 5429 citation relationships (edges). \n", "\n", "The dataset is located in the `data/cora` directory and contains 3 main files:\n", "1. `README.txt`: Provides an overall description of the CORA dataset.\n", "2. `cora.content`: Contains attributes and the label specific to every paper in the dataset. \n", "3. `cora.cites`: Contains the data that is required to generate the citation relationships."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### `cora.content` File\n", "\n", "This file contains a multitude of attributes that are specific to each paper, along with the target label. The attributes are extracted from words that occur in the papers. After removing stopwords and stemming the word tokens, tokens that occurred less than 10 times were removed. This has led to a word vocabulary of 1,433 unique word tokens that represent the attributes of the paper. \n", "\n", "The `.content` file contains descriptions of the papers in the following format (where `\\t` is a tab):\n", "- `<paper_id>\\t<word_attributes_1>` ... `<word_attributes_1433>\\t<class_label>`\n", "\n", "We load this file first."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Import useful libraries\n", "import pandas as pd\n", "from sklearn.utils import shuffle"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["PAPER_ID_COL = \"paper_id\"\n", "WORD_TOKEN_COLS = [\"word_{}\".format(i) for i in range(1433)]\n", "LABEL_COL = \"label\"\n", "\n", "CONTENT_COLS = [PAPER_ID_COL] + WORD_TOKEN_COLS + [LABEL_COL]\n", "\n", "def read_cora_content(filepath):\n", "    \"\"\" \n", "    Reads cora.content file and puts the data into a pandas.DataFrame after \n", "    shuffling the observations\n", "    \"\"\"\n", "    data = shuffle(pd.read_csv(filepath, sep=\"\\t\", names=CONTENT_COLS), random_state=42)\n", "    data.reset_index(drop=True, inplace=True)\n", "    \n", "    return data"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["content_df = read_cora_content(\"data/cora/cora.content\")\n", "content_df"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### `cora.cites` File\n", "\n", "This file contains the citation graph of the corpus. The cited and citing papers are connected with an edge.\n", "\n", "The `.cites` file contains lines where each line describes a citation link in the following format:\n", "- `<paper_id of cited paper>\\t<paper_id of citing paper>`\n", "\n", "Second, we load the `cora.cites` file. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["CITES_COLS = [\"cited_paper\", \"citing_paper\"]\n", "\n", "def read_cora_cites(filepath):   \n", "    \"\"\" Reads cora.cites file and puts the data into a pandas.DataFrame\n", "    \"\"\"\n", "    data = pd.read_csv(filepath, sep=\"\\t\", names=CITES_COLS)\n", "    \n", "    return data"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["cites_df = read_cora_cites(\"data/cora/cora.cites\")\n", "cites_df"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Allocating Training, Validation, and Test sets\n", "\n", "In the original [GCN paper](https://arxiv.org/abs/1609.02907), 20 examples from each class are used as testing data. However in this tutorial, we follow a more popular and tested [hold-out validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) approach, that uses training, validation, and test datasets. \n", "\n", "The `get_train_val_test_datasets` function below takes the dataset and splits it into 3 distinct sets of observations that can be used for training, validation, and testing."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "def get_train_val_test_datasets(df, test_frac=.3, val_frac=.3):\n", "    \"\"\" \n", "    Splits the full dataset into 3 mutually exclusive observations sets for Training, Validation\n", "    and Testing purposes. The splitting is also stratified meaning that the label distribution is\n", "    preserved accross the three datasets.\n", "    \n", "    Params: \n", "        df (pandas.DataFrame): DataFrame containing the attributes of research papers\n", "        test_frac (float): The fraction of full dataset that should be allocated for testing\n", "        val_frac (float): The fraction of the training dataset that should be allocated for validation\n", "        \n", "    Returns:\n", "        train (pandas.DataFrame): DataFrame containing the training observations\n", "        validation (pandas.DataFrame): DataFrame containing the validation observations\n", "        test (pandas.DataFrame): DataFrame containing the test observations\n", "    \"\"\"\n", "    # split the full dataset to train and test\n", "    train, test = train_test_split(df, \n", "                                   test_size=test_frac, \n", "                                   random_state=42, \n", "                                   stratify=content_df[LABEL_COL])\n", "    \n", "    # further split the training dataset to train and validation\n", "    train, validation = train_test_split(train, \n", "                                         test_size=val_frac, \n", "                                         random_state=42, \n", "                                         stratify=train[LABEL_COL])\n", "    \n", "    return train, validation, test"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We use 70% of the full dataset for training and validation while 30% of the data is used for testing. The training dataset is also further split to training and validation sets with a 70:30 proportion."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["train_data, validation_data, test_data = get_train_val_test_datasets(content_df, \n", "                                                                     test_frac=.3, \n", "                                                                     val_frac=.3) "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We can empirically evaluate the effect of stratification by looking at the label distribution across the three datasets. \n", "\n", "We can start by displaying the label distribution on training data."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["print(\"The label distribution in the training data is as follows:\")\n", "display(train_data[LABEL_COL].value_counts() / len(train_data))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Exercise\n", "\n", "Check the label distribution in the `validation_data` and `test_data` programatically to make sure the label distributions are similar."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Validation data\n", "print(\"The label distribution in the validation data is as follows:\")\n", "\n", "# Your code here...\n", "display(validation_data[LABEL_COL].value_counts()/len(validation_data))\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Test data\n", "print(\"The label distribution in the test data is as follows:\")\n", "\n", "# Your code here...\n", "display(test_data[LABEL_COL].value_counts()/len(test_data))\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Preparing Data for Machine Learning\n", "\n", "Preparing data for training a machine learning algorithm entails a few additional tasks apart from loading the source data and cleaning it. Sometimes, the raw data that we find is not in a suitable numerical format that is aligned with the machine learning libraries that we intend to use them with. \n", "\n", "In such scenarios, additional preparations need to be done. In the following section, we carry out a few such steps."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Encoding Labels\n", "\n", "The task at hand is a supervised learning task where the topic of the publication has to be predicted based on the available features. The label in our task, topics, is a discrete categorical variable and needs to be converted into a numerical representation. \n", "\n", "Different approaches exist in order to [vectorise a categorical variable](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features). However, in this case, _one-hot-encoding_ is a suitable approach to convert our categorical label into a numeric one."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Exercise\n", "\n", "Use `sklearn.preprocessing.OneHotEncoder` found [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn-preprocessing-onehotencoder) to convert the label into a one-hot encoding. \n", "\n", "Implement your logic in the `to_one_hot_encoding` function below.\n", "\n", "***Hint***: The attributes available in the `OneHotEncoder` class allow you to obtain the label mapping."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Import relevant libraries\n", "\n", "# Your code here...\n", "from sklearn.preprocessing import OneHotEncoder\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Implement the function below\n", "def to_one_hot_encoding(label_col):\n", "    \"\"\" \n", "    Returns a one hot encoding of the labels with the index-label mapping\n", "    \n", "    Params:\n", "        label_col (pandas.Series): a pandas series with target label (str) for each observation\n", "        \n", "    Returns:\n", "        label_mapping ([str]): A list of class labels where the index of the label corresponds to\n", "                                the position of label in the one-hot vector\n", "        labels (np.array): A 2-d array containing the one-hot encoded labels from dataset\n", "    \"\"\"\n", "    # Your code here...\n", "    y = [[_y] for _y in label_col]\n", "    label_encoder = OneHotEncoder()\n", "    labels = label_encoder.fit_transform(y)\n", "    label_mapping = label_encoder.categories_[0]\n", "    return label_mapping, labels\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We use the `to_one_hot_encoding` function to obtain the encoded labels and the label mapping."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["label_index, one_y = to_one_hot_encoding(content_df[LABEL_COL])"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["print(label_index)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Vectorising Node Features\n", "\n", "We must also extract each node's features from the data. Luckily the CORA dataset already provides numerical features. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["X = content_df[WORD_TOKEN_COLS]\n", "test_data"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We also calculate a few other statistics which might be useful. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["nodes = content_df[\"paper_id\"]\n", "n_obs = len(nodes)\n", "n_feat = len(WORD_TOKEN_COLS)\n", "n_labels = content_df[LABEL_COL].nunique()\n", "\n", "print(f\"There are {n_obs} nodes, each with {n_feat} features and {n_labels} total classes.\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Part 2: Machine Learning with Graphs\n", "\n", "In this section, we focus on developing machine learning models with the CORA dataset. We will attempt to build two neural networks to predict the topic label of the papers in the CORA dataset.\n", "\n", "As a first attempt, we will build a feed-forward neural network (FNN), a conventional neural network that only considers the node features in predicting the label. \n", "\n", "As an evolution, we will then build a graph convolutional neural network (GCN), which will allow us to leverage the graph that is available to us. \n", "\n", "In this scenario, we use `spektral`, a Python library that has a Keras-like interface for GNN development. With `spektral`, we use `tensorflow` as our deep neural network computation library. In order to make sure that the code runs successfully in `tensorflow`, we need to create a few data structures that will allow us to use our dataset with `tensorflow` to develop DNNs. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Transforming Data for the `tensorflow.keras` Interface\n", "\n", "Before we start building neural network models, further transformations are necessary for the data structures we use to be compliant with the Python ML libraries we are using."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Masks\n", "\n", "Masks allow us to use the entire dataset as one matrix, while allowing us to specifically select subsets of data during runtime to work with.\n", "\n", "A great utilisation of masks is to select training, validation, and test observations from the full dataset without having to create multiple different datasets at the start.\n", "\n", "In the following cell, we create three masks that will be used for training, validating, and testing the models we build in `tensorflow`. As you will see in subsequent steps, these steps can be used to define the `sample_weight` parameter in `tensorflow`.  "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import numpy as np\n", "\n", "# Set masks \n", "train_mask = np.zeros((n_obs,), dtype=bool)\n", "train_mask[train_data.index] = True\n", "\n", "validation_mask = np.zeros((n_obs,), dtype=bool)\n", "validation_mask[validation_data.index] = True\n", "\n", "test_mask = np.zeros((n_obs,), dtype=bool)\n", "test_mask[test_data.index] = True"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Compatible Data Types\n", "\n", "Having compatible data types is also important in `tensorflow`, and leads to having fewer runtime errors. We convert both `X` (node features) and `one_y` (labels) to the `float32` data type.  "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["X = X.to_numpy().astype(\"float32\")\n", "one_y = one_y.toarray().astype(\"float32\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Feed-forward Network to Predict Paper Topic\n", "\n", "As the first step, we will build a feed-forward neural network, which is one of the most foundational architectures of the neural network model. In this model, no graph-related data is taken into consideration. The prediction model is learned solely based on the features that are associated with the nodes in the graph (1433 word attributes of the features in this scenario). \n", "\n", "### Parts of a Neural Network\n", "\n", "A neural network consists of many different building blocks that can be assembled together in order to create a model. These blocks can be various components such as *hidden layers*, *activations*, *regularisers*, and other types of blocks."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Example: Single Layer Neural Network\n", "\n", "As the first example, we build a *single layer neural network* which consists of ***one*** hidden layer that maps the input features to the labels. The following figure illustrates the network architecture. \n", "\n", "<img src=\"img/shallow_net.jpg\" class=\"wide\">"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["In this architecture, we build a single layer neural network. The network takes the features as input (`X` in the figure) and learns a single layer of weights that uses a `softmax` activation.\n", "\n", "As seen, this network uses a neural layer, which is a `Dense` layer, that has an `activation` function. Implementing this network in the `tensorflow` library will require using:\n", "1. the [`Input` interface](https://www.tensorflow.org/api_docs/python/tf/keras/Input), that defines the input to the network (features)\n", "2. a [Dense layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense), where the parameters are learned\n", "\n", "This `Dense` layer should also be equipped with [`L2` regularisation](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2) to combat overfitting.\n", "\n", "We also use the [`EarlyStopping` approach](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) to combat overfitting in a global level (full model). \n", "\n", "As this is the final `Dense` layer that maps the values to the target labels (`y`), it is called the ***Output Layer***. \n", "\n", "We start by importing the relevant classes and setting up hyperparameters."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from tensorflow.keras.layers import Input, Dense, Dropout # model components\n", "\n", "# Import regularisers\n", "from tensorflow.keras.regularizers import l2 \n", "from tensorflow.keras.callbacks import EarlyStopping \n", "\n", "from tensorflow.keras.models import Model # to store the entire model"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["l2_reg = 5e-4"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Architecture Definition\n", "\n", "As per the figure above, we define the model architecture."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Input interface with 1433 features\n", "X_in = Input(shape=(n_feat, ))\n", "\n", "# Hidden layer with softmax activation and \n", "layer_1 = Dense(\n", "    n_labels,\n", "    activation='softmax',\n", "    kernel_regularizer=l2(l2_reg)\n", ")(X_in)\n", "\n", "# Instantiate the model\n", "model_1_layer = Model(inputs=X_in,\n", "                      outputs=layer_1)\n", "\n", "# Validate if the architecture is sound\n", "model_1_layer.summary()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Model Training\n", "\n", "Once the architecture is defined successfully, we need to define the optimisation algorithm we want to use in order to learn the parameters. \n", "\n", "We use [`Adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) for this, which is a modified version of vanilla stochastic gradient descent. We use [categorical cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) as the loss function."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from tensorflow.keras.optimizers import Adam # import optimisation algorithm"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Setting up hyperparameters for the model\n", "es_patience = 10\n", "learning_rate = 1e-2 # learning rate for the optmiser\n", "epochs = 200 # no. of iterations\n", "es_patience = 10 #\u00a0no. of epochs to wait before stopping if performance keeps decreasing"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["optimizer = Adam(learning_rate=learning_rate)\n", "\n", "model_1_layer.compile(\n", "    optimizer=optimizer,\n", "    loss='categorical_crossentropy',\n", "    weighted_metrics=['acc']\n", ")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Before training the model, we should also prepare the validation dataset that is used to identify the right combination of parameters for the model. [Early stopping](https://en.wikipedia.org/wiki/Early_stopping) allows the algorithm to stop training the model when a good solution is found. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Prepare validation data\n", "validation_data_fnn = (X, one_y, validation_mask)\n", "\n", "# Train model\n", "model_1_layer.fit(\n", "    X,\n", "    one_y,\n", "    sample_weight=train_mask,\n", "    epochs=epochs,\n", "    batch_size=n_obs,\n", "    validation_data=validation_data_fnn,\n", "    shuffle=False,\n", "    callbacks=[\n", "        EarlyStopping(patience=es_patience,  restore_best_weights=True)\n", "    ]\n", ")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The trained model can be evaluated with the test data to evaluate the predictive performance of this model."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import classification_report\n", "\n", "X_test = X[test_mask]\n", "y_test = one_y[test_mask]\n", "\n", "y_test_pred = model_1_layer.predict(X_test)\n", "report = classification_report(\n", "    np.argmax(y_test,axis=1),\n", "    np.argmax(y_test_pred,axis=1), \n", "    target_names=label_index.tolist()\n", ")\n", "\n", "print(f'Simple Neural Network Classification Report: \\n {report}')"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Deep Neural Network (Another Feed-forward Neural Network)\n", "\n", "Now that we have familiarised ourselves with a single layer neural network and how it is implemented in `tensorflow`, we build a more sophisticated neural architecture to learn a more complex model. The proposed model architecture for the new model has multiple layers and looks like this. \n", "\n", "<img src=\"img/deep_net.jpg\" class=\"wide\">"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Exercise \n", "\n", "Implement the code to build the above model architecture using `tensorflow`. \n", "\n", "You will need to use the following classes:\n", "1. [`Input` interface](https://www.tensorflow.org/api_docs/python/tf/keras/Input)\n", "2. [`Dense` layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)\n", "3. [`Dropout` regularisation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)\n", "\n", "Build a model (`Model` class type) named `model_fnn` based on these components. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Setting up hyperparameters for the model\n", "es_patience = 10\n", "l2_reg = 5e-4\n", "learning_rate = 1e-2\n", "epochs = 200\n", "dropout = 0.5"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Implement the model architecture\n", "\n", "# Your code here...\n", "X_in = Input(shape=(n_feat, ))\n", "\n", "layer_h1 = Dense(\n", "    128,\n", "    activation='relu',\n", "    kernel_regularizer=l2(l2_reg)\n", ")(X_in)\n", "\n", "dropout_1 = Dropout(dropout)(layer_h1)\n", "layer_h2 = Dense(\n", "    256, \n", "    activation='relu'\n", ")(dropout_1)\n", "\n", "dropout_2 = Dropout(dropout)(layer_h2)\n", "\n", "layer_output = Dense(\n", "    n_labels,\n", "    activation='softmax'\n", ")(dropout_2)\n", "\n", "model_fnn = Model(\n", "    inputs=X_in,\n", "    outputs=layer_output\n", ")\n", "\n", "model_fnn.summary()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["optimizer = Adam(learning_rate=learning_rate)\n", "\n", "model_fnn.compile(\n", "    optimizer=optimizer,\n", "    loss='categorical_crossentropy',\n", "    weighted_metrics=['acc']\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Create validation dataset\n", "validation_data_fnn = (X, one_y, validation_mask)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Use the `.fit` method with the relevant parameters to train the model, while using early stopping to stop training if the model has found a good solution. \n", "\n", "***Hint***: You may draw from the previous example where the `.fit` method was implemented for the shallow learning network."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Train model\n", "\n", "# Your code here...\n", "model_fnn.fit(\n", "    X,\n", "    one_y,\n", "    sample_weight=train_mask,\n", "    epochs=epochs,\n", "    batch_size=n_obs,\n", "    validation_data=validation_data_fnn,\n", "    shuffle=False,\n", "    callbacks=[\n", "        EarlyStopping(patience=es_patience,  restore_best_weights=True)\n", "    ]\n", ")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import classification_report"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Evaluate model\n", "X_test = X[test_mask]\n", "y_test = one_y[test_mask]\n", "\n", "y_test_pred = model_fnn.predict(X_test)\n", "report = classification_report(\n", "    np.argmax(y_test,axis=1),\n", "    np.argmax(y_test_pred,axis=1), \n", "    target_names=label_index.tolist()\n", ")\n", "\n", "print(f'FNN Classification Report: \\n {report}')"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Creating the Adjacency Matrix\n", "\n", "In order to create a graph neural network, we need to create the adjacency matrix of the graph. We can use the above loaded data to do this. In this particular case, let us treat this graph to be an *undirected* graph, to keep the implementation simple (although the directions do exist).\n", "\n", "Let us now use `NetworkX` to create the graph representation of the CORA dataset. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import networkx as nx\n", "\n", "cora_graph = nx.Graph()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Exercise\n", "\n", "Use the `content_df` and `cites_df` DataFrames that were created in part 1 to add nodes and edges to `cora_graph`. \n", "\n", "***Hint***: You may use the `add_node()` and `add_edge()` methods from part 1 to add nodes and edges."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Add all the nodes to the dataset\n", "\n", "# Your code here...\n", "for node in nodes:\n", "    cora_graph.add_node(node)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Add all the edges\n", "\n", "# Your code here...\n", "for _, edge in cites_df.iterrows():\n", "    cora_graph.add_edge(\n", "        edge[\"cited_paper\"],\n", "        edge[\"citing_paper\"],\n", "        weight=1.0\n", "    )\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Now we can visualise the CORA graph (this may take around 30 seconds)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["display_options = {\n", "    'node_size': 30,\n", "    'width': 0.2,\n", "    'with_labels': False\n", "}\n", "\n", "nx.draw(cora_graph, **display_options)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Use the [`adjacency_matrix()`](https://networkx.org/documentation/stable/reference/generated/networkx.linalg.graphmatrix.adjacency_matrix.html) utility function available in `NetworkX` to create the sparse matrix that represents the adjacency matrix of graph `cora_graph`.\n", "\n", "Assign this to a variable called `A`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Retrieve the adjacency matrix A\n", "\n", "# Your code here...\n", "A = nx.adjacency_matrix(cora_graph)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["print(A)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Graph Convolutional Neural Network with `spektral`\n", "\n", "As the final step, we build a graph neural network. In this exercise, we will build a specific type of graph neural network that uses a block called Graph Convolutional Network (GCN), which is proposed in a recent [research paper](https://arxiv.org/abs/1609.02907). \n", "\n", "We use the library [`spektral`](https://graphneural.network/) to implement this network due to its compatibility with `tensorflow` and user friendliness. \n", "\n", "### Architecture\n", "\n", "The proposed architecture for the CORA dataset is illustrated in the figure below:\n", "\n", "<img src=\"img/graph_net.jpg\" class=\"wide\">\n", "\n", "There are a few differences here in comparison to the other networks we saw earlier. \n", "1. Presence adjacency matrix (Orange box titled `Adj`)\n", "2. Presence of `GCN Conv` layers instead of hidden layers.\n", "\n", "As this is a graph neural network architecture, it exploits the graph structure of the data. The [`GCNConv` layer](https://graphneural.network/layers/convolution/#gcnconv) takes care of this by taking the adjacency matrix of the graph as an input parameter. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Exercise\n", "\n", "Implement the architecture outlined above using `spektral` and `tensorflow`. The architecture is inspired by this [web post](https://towardsdatascience.com/graph-convolutional-networks-on-node-classification-2b6bbec1d042), which also demonstrates how `spektral` and `tensorflow` can operate together. \n", "\n", "***Hints***: \n", "- You may use `Input`, `Dropout`, and `Model` components from `tensorflow` library\n", "- The `GCNConv` layer can be imported from the `spektral` library where extensive [documentation](https://graphneural.network/layers/convolution/#gcnconv) relating to this layer is available.\n", "- As per the [documentation](https://graphneural.network/layers/convolution/#gcnconv), the original adjacency matrix cannot be used as it is, and should be modified (into the Laplacian). Use the instructions in the documentation to carry out this procedure.\n", "- Set the `use_bias` parameter to `False` in both `GCNConv` layers\n", "- The `GCNConv` layer instance needs two inputs as per the figure above.\n", "    - Embedding from the previous layer\n", "    - Transformed adjacency matrix \n", "- You can pass the two inputs to the `GCNConv` layer as a list of the two objects.\n", "    - eg: \n", "    ```\n", "    gcn_layer = GCNConv()\n", "    gcn_embedding = gcn_layer([prev_layer_output, trans_adj_matrix])  \n", "    ```"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["***Step 1***: Preprocess the adjacency matrix `A` as per the instructions in the [API documentation](https://graphneural.network/layers/convolution/#gcnconv) to create a new variable `_Adj`"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "from spektral.utils.convolution import gcn_filter\n", "\n", "_Adj = gcn_filter(A)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We convert the `_Adj` variable to floating point values, renaming it to `Adj`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["Adj = _Adj.astype('f4')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["print(Adj)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["***Step 2***: Build architecture and train, calling your model `model_gnn`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# GCNConv layer from spektral\n", "from spektral.layers import GCNConv"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Set hyperparameters\n", "channels = 16           # Number of channels in the first layer\n", "dropout = 0.5           # Dropout rate for the features\n", "l2_reg = 5e-4           # L2 regularisation rate\n", "learning_rate = 1e-2    # Learning rate\n", "epochs = 200            # Number of training epochs\n", "es_patience = 10        # Patience for early stopping"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Define architecture\n", "# Your code here...\n", "X_in = Input(shape=(n_feat, )) # input features from the nodes\n", "fltr_in = Input(shape=(n_obs, ), sparse=True) # filter defined by the graph structure\n", "\n", "dropout_1 = Dropout(dropout)(X_in)\n", "graph_conv_1 = GCNConv(\n", "    channels,\n", "    activation='relu',\n", "    kernel_regularizer=l2(l2_reg),\n", "    use_bias=False\n", ")([dropout_1, fltr_in])\n", "\n", "dropout_2 = Dropout(dropout)(graph_conv_1)\n", "graph_conv_2 = GCNConv(\n", "    n_labels,\n", "    activation='softmax',\n", "    use_bias=False\n", ")([dropout_2, fltr_in])\n", "\n", "model_gnn = Model(\n", "    inputs=[X_in, fltr_in],\n", "    outputs=graph_conv_2\n", ")\n", "\n", "model_gnn.summary()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["optimizer = Adam(learning_rate=learning_rate)\n", "\n", "model_gnn.compile(\n", "    optimizer=optimizer,\n", "    loss='categorical_crossentropy',\n", "    weighted_metrics=['acc']\n", ")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We prepare the validation data and train the model. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["validation_data = ([X, Adj], one_y, validation_mask)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Train model\n", "model_gnn.fit(\n", "    [X, Adj],\n", "    one_y,\n", "    sample_weight=train_mask,\n", "    epochs=epochs,\n", "    batch_size=n_obs,\n", "    validation_data=validation_data,\n", "    shuffle=False,\n", "    callbacks=[\n", "        EarlyStopping(patience=es_patience,  restore_best_weights=True)\n", "    ]\n", ")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Using Test Data\n", "\n", "Now that we have trained our graph neural network, we want to evaluate its performance with test data by using the held-out test dataset. However, this is a bit tricky with a GNN as the features and the connection information from the training data are also used as features. \n", "\n", "Therefore, we run a prediction on the whole dataset, including the entire adjacency matrix, before using the mask to obtain results just for the test set."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Predict on all the nodes in the entire graph\n", "y_test_pred_all = model_gnn.predict([X, Adj], batch_size=n_obs)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Use the mask to filter out the true and predicted labels, and verify that they are the same dimension\n", "y_test = one_y[test_mask]\n", "y_test_pred = y_test_pred_all[test_mask,:]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["y_test_pred.shape"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["y_test.shape"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Evaluate classification performance\n", "report = classification_report(\n", "    np.argmax(y_test,axis=1),\n", "    np.argmax(y_test_pred,axis=1), \n", "    target_names=label_index.tolist()\n", ")\n", "\n", "print(f'GCN Classification Report: \\n {report}')"]}]}
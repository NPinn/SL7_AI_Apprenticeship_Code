{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 4, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Part 3: Pretrained Convolutional Neural Networks"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["%matplotlib inline"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "# for elementary image manipulation\n", "import imageio as iio\n", "\n", "# specifies the default figure size for this notebook\n", "plt.rcParams['figure.figsize'] = (10, 10)\n", "\n", "# specifies the default color map\n", "plt.rcParams['image.cmap'] = 'gray'\n", "\n", "# we'll use keras to build our networks\n", "from tensorflow import keras\n", "from keras.models import Sequential\n", "\n", "from keras.layers import Flatten, Dense, Dropout\n", "from keras.layers import Conv2D, MaxPooling2D\n", "from keras.layers import ZeroPadding2D\n", "\n", "from tensorflow.keras.optimizers import SGD"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Working with a Pre-Trained Network\n", "\n", "Now lets load an already trained network in our environment. This network (VGG-16) has been trained on the Imagenet dataset where the goal is to classify pictures into one out of one thousand categories. \n", "When it came out in 2014, it won the annual ImageNet Recognition Challenge correctly classifying 93% of the images in the test set. \n", "For comparison, humans can achieve around 95% accuracy. \n", "It's also very simple, it only uses 3x3 convolutions (like the ones you have used before)! \n", "\n", "It is however rather deep and it takes **2 to 3 weeks with 4 GPUs** to train it...\n", "\n", "To load the model, you must first define it's architecture. \n", "You're going to do this step by step as you learn the components of convolutional neural networks. \n", "\n", "**IMPORTANT NOTE**: it is extremely difficult to come up with an architecture \"that works\". So while people successfully adapt existing neural nets such as VGG16 to their needs (and, in fact, a lot of people use it without its last layer as an *image-embedding operator*), architecture design is the realm of research (and much head scratching).\n", "\n", "Recently, the Google Brain team has applied brute-force style search to try to find the best architectures for problems but also to learn what activation function to use, what step-size mechanisms etc (learn everything-approach). \n", "This required an obscene amount of resources and the results were far from intuitive (*search for \"Google Brain 2017 year review\" for a discussion of this and many other interesting results*). "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Implementing a convolutional layer\n", "\n", "You are going to define the first convolutional layer of the network. But before, you will add some padding to the image so the convolutions get to apply on the outer edges.\n", "\n", "In what follows you don't have to modify the cells but just run them making sure you understand what is being done. Do not tune the parameters as we will load pre-trained weights on the architecture!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Create the model, it's a Sequential model (stack of layers one after the other)\n", "vgg_model = Sequential()\n", "\n", "# On the very first layer, you must specify the input shape\n", "# ZeroPadding2D adds a frame of 0 (column left and right, row top and bottom)\n", "# the tuple (1, 1) indicates it's one pixel and symmetric.\n", "vgg_model.add(ZeroPadding2D((1, 1), input_shape=(224, 224, 3))) \n", "\n", "# Your first convolutional layer will have 64 3x3 filters, \n", "# and will use a relu activation function\n", "vgg_model.add(Conv2D(64, (3, 3), activation='relu', name='conv1_1'))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Stacking layers\n", "\n", "Now you're going to stack another convolutional layer. Remember, the output of a convolutional layer is a 3-D tensor, just like the input image. Although it does have a much higher depth!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Once again you must add padding\n", "vgg_model.add(ZeroPadding2D((1, 1)))\n", "vgg_model.add(Conv2D(64, (3, 3), activation='relu', name='conv1_2'))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Adding pooling layers\n", "\n", "Now lets add your first pooling layer. Pooling reduces the width and height of the input by aggregating adjacent cells together.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Add a pooling layer with window size 2x2\n", "# The stride indicates the distance between each pooled window\n", "vgg_model.add(MaxPooling2D((2, 2), strides=(2, 2)))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Adding more convolutions for VGG\n", "\n", "Now you can stack many more of these! Remember not to change the parameters as we are about to load the weights of an already trained version of this network.\n", "\n", "Also, as you will quickly realise, Keras for practitioners usually means a lot of copy-pasting..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# second set of Padding - Conv - Padding - Conv - Pooling\n", "vgg_model.add(ZeroPadding2D((1, 1)))\n", "vgg_model.add(Conv2D(128, (3, 3), activation='relu', name='conv2_1'))\n", "vgg_model.add(ZeroPadding2D((1, 1)))\n", "vgg_model.add(Conv2D(128, (3, 3), activation='relu', name='conv2_2'))\n", "vgg_model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n", "\n", "# third set\n", "vgg_model.add(ZeroPadding2D((1, 1)))\n", "vgg_model.add(Conv2D(256, (3, 3), activation='relu', name='conv3_1'))\n", "vgg_model.add(ZeroPadding2D((1, 1)))\n", "vgg_model.add(Conv2D(256, (3, 3), activation='relu', name='conv3_2'))\n", "vgg_model.add(ZeroPadding2D((1, 1)))\n", "vgg_model.add(Conv2D(256, (3, 3), activation='relu', name='conv3_3'))\n", "vgg_model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n", "\n", "# fourth set\n", "vgg_model.add(ZeroPadding2D((1, 1)))\n", "vgg_model.add(Conv2D(512, (3, 3), activation='relu', name='conv4_1'))\n", "vgg_model.add(ZeroPadding2D((1,1)))\n", "vgg_model.add(Conv2D(512, (3, 3), activation='relu', name='conv4_2'))\n", "vgg_model.add(ZeroPadding2D((1, 1)))\n", "vgg_model.add(Conv2D(512, (3, 3), activation='relu', name='conv4_3'))\n", "vgg_model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n", "\n", "# fifth set\n", "vgg_model.add(ZeroPadding2D((1, 1)))\n", "vgg_model.add(Conv2D(512, (3, 3), activation='relu', name='conv5_1'))\n", "vgg_model.add(ZeroPadding2D((1, 1)))\n", "vgg_model.add(Conv2D(512, (3, 3), activation='relu', name='conv5_2'))\n", "vgg_model.add(ZeroPadding2D((1, 1)))\n", "vgg_model.add(Conv2D(512, (3, 3), activation='relu', name='conv5_3'))\n", "vgg_model.add(MaxPooling2D((2, 2), strides=(2, 2)))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["As you can see, the depth of the layers get progressively larger, up to 512 for the latest layers. \n", "This means that, as we go along, each layer detects a greater number of features. \n", "\n", "On the other hand, each max-pooling layer halves the height and width of the layer outputs. \n", "Starting from images of dimensions 224x224, the final outputs are only of size 7x7.\n", "\n", "Now you're about to add some fully connected layers which can learn the more abstract features of the image. \n", "But first you must first change the layout of the input so it looks like a 1-D tensor (vector)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Flatten the output\n", "vgg_model.add(Flatten())\n", "\n", "# Add a fully connected layer with 4096 neurons\n", "vgg_model.add(Dense(4096, activation='relu'))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The `Flatten` function removes the spatial dimensions of the layer output, it is now a simple 1-D vector of numbers. This means we can no longer apply 2D convolution layers as before, but we can apply fully connected layers like the ones of the perceptron from the previous module.\n", "\n", "`Dense` layers are fully connected layers. You used them in the previous module.\n", "\n", "### Preventing overfitting with Dropout\n", "\n", "`Dropout` is a method used at train time to prevent overfitting. As a layer, it randomly modifies its input\n", "so that the neural network learns to be robust to these changes. Although you won\u2019t actually use it\n", "now, you must define it to correctly load the pre-trained weights as it was part of the original network."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Add a dropout layer\n", "vgg_model.add(Dropout(0.5))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The number 0.5 indicates the amount of change, 0.0 means no change, and 1.0 means completely diff\u0081erent.\n", "\n", "\n", "Add one more fully connected layer:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["vgg_model.add(Dense(4096, activation='relu'))\n", "vgg_model.add(Dropout(0.5))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Finally a softmax layer to predict the categories. There are 1000 categories and hence 1000 neurons."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["vgg_model.add(Dense(1000, activation='softmax'))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Loading the weights\n", "\n", "And you're all set with the architecture! Let's load the weights of the network. For this use \n", "\n", "```python\n", "vgg_model.load_weights(path)\n", "```\n", "\n", "where `path` is the path to `vgg16_weights_tf_dim_ordering_tf_kernels.h5`. Those are the weights for the pre-trained VGG16, they can be downloaded online so you don't need to re-train the model yourself.\n", "\n", "The weights file need to be downloaded to the `data` folder as follow:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import urllib.request\n", "\n", "url = 'https://info.cambridgespark.com/hubfs/Curriculum%20Team%20Folder/L7%20AI/Neural%20Networks/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n", "output_path = 'data/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n", "urllib.request.urlretrieve(url, output_path)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["weight_loc = 'data/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n", "vgg_model.load_weights(weight_loc)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Compile the network no need to worry about this for now"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["sgd = SGD()\n", "vgg_model.compile(optimizer=sgd, loss='categorical_crossentropy')"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Preprocessing the data\n", "\n", "Lets feed an image to your model. In the VGG network, we only do zero centering. The model takes as input a slightly transformed version of the input. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Load an image of a cat and a puppy (feel free to test with another one of your images)\n", "img = iio.imread('data/cat.jpg')\n", "img2 = iio.imread('data/puppy.jpg')\n", "\n", "# Plot the images to check them\n", "plt.imshow(img)\n", "plt.axis('off')\n", "plt.figure()\n", "plt.imshow(img2)\n", "plt.axis('off')\n", "plt.figure()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The VGG16 network assumes that the input it will receive is an array of 224 by 224 RGB images that have been *centered* in each of their channels. \n", "\n", "The function below applies the centering and makes sure the dimensions are right. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# This transformation performs the 0-centering\n", "def transform_image(image):\n", "    image_t = np.copy(image)\n", "    image_t = keras.preprocessing.image.smart_resize(\n", "        image_t, (224, 224), interpolation='bilinear'\n", "    )\n", "\n", "    image_t = image_t.astype(np.float32) # Avoids modifying the original\n", "    image_t[:, :, 0] -= 103.939                 # Substracts mean Red\n", "    image_t[:, :, 1] -= 116.779                 # Substracts mean Green\n", "    image_t[:, :, 2] -= 123.68                  # Substracts mean Blue\n", "    image_t = np.expand_dims(image_t, axis=0)   # The network takes batches of images as input\n", "    return image_t\n", "\n", "img_t = transform_image(img)\n", "img2_t = transform_image(img2)\n", "\n", "print(img_t.shape)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The first dimension is a \"dummy\" dimension, that is because the network expects an array of images as input. \n", "\n", "The three subsequent dimensions are the image dimensions with the three colour channels at the end. \n", "\n", "### Getting an output from the network\n", "\n", "Let's push the images through the network and see what happens!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Push the image through the network using vgg_model.predict call the result\n", "out = vgg_model.predict(img_t)\n", "out2 = vgg_model.predict(img2_t)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# the output is for a batch of images but you only gave one so extract the first element\n", "out = out[0]                   \n", "out2 = out2[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# now plot the output, xlabel=Categories, ylabel=Probabilities\n", "plt.figure()\n", "plt.ylabel('Probabilities')\n", "plt.xlabel('Categories')\n", "cats = np.arange(out.shape[0])\n", "plt.vlines(cats, [0], out, label='out', color='C0')\n", "plt.vlines(cats, [0], out2, label='out2', color='C1')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The network seems pretty confident! Lets look at its top 5 guesses for each of the two images. \n", "\n", "1. load the labels from `data/synset_words`\n", "2. sort the top 5 probabilities\n", "3. display the corresponding categories with their predicted probabilities"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Load labels\n", "imagenet_labels_filename = 'data/synset_words.txt'\n", "labels = np.loadtxt(imagenet_labels_filename, str, delimiter='\\t')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["top_5 = out.argsort()[-5:][::-1]\n", "top_5_values = np.sort(out)[-5:][::-1]\n", "print('Image 1')\n", "for label, prob in zip(labels[top_5], top_5_values):\n", "    print('label: {} with probability: {:0.3f}'.format(label, prob))\n", "\n", "top2_5 = out2.argsort()[-5:][::-1]\n", "top2_5_values = np.sort(out2)[-5:][::-1]\n", "\n", "print('\\nImage 2')\n", "for label, prob in zip(labels[top2_5], top2_5_values):\n", "    print('label: {} with probability: {:0.3f}'.format(label, prob))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Not too bad...\n", "\n", "### Looking inside the network\n", "\n", "In a convolutional neural network, there's an easy way to visualise the filters learned at the very first layer. We can print each filter to show which colours it reponds to."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# This is a helper function to let you visualise what goes on inside the network\n", "\n", "def vis_square(weights, padsize=1, padval=np.nan, activation=False,\n", "               normalize=True, diverging=True, cmap=None):\n", "    # Avoids modifying the network weights\n", "    data = np.copy(weights)\n", "    \n", "    if normalize:\n", "        # Normalise the inputs - this makes sense for interpreting colour\n", "        # -- we are interested in relative difference between patches\n", "        # however, as with activations, we may sometimes want to understand\n", "        # want to understand the magnitude of the values (i.e. un-normalized)\n", "        data -= data.min()\n", "        data /= data.max()\n", "    \n", "    \n", "    vmax = int(\n", "        max(\n", "            np.abs(np.max(data)),\n", "            np.abs(np.min(data))\n", "        )\n", "    )\n", "    vmin = -vmax\n", "    \n", "    # Lets tile the inputs\n", "    # How many inputs per row (e.g.: if 64 filters then 8x8)\n", "    \n", "    n = int(np.ceil(np.sqrt(data.shape[0]))) \n", "    \n", "    # add padding between inputs (you can safely ignore this)\n", "    padding = ((0, n**2 - data.shape[0]), (0, padsize), (0, padsize)) + ((0, 0),) * (data.ndim - 3)\n", "    data = np.pad(data, padding, mode='constant', constant_values=(padval, padval))\n", "    \n", "    # place the filters on an n by n grid\n", "    data = data.reshape((n, n) + data.shape[1:])\n", "    \n", "    # merge the filters contents onto a single image\n", "    data = data.transpose((0, 2, 1, 3) + tuple(range(4, data.ndim)))\n", "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n", "    \n", "    # complete the border\n", "    data = np.pad(data, [(padsize, 0), (padsize, 0), (0, 0)], mode='constant', \n", "                  constant_values=(padval))\n", "    \n", "    # show the filter. In the activation case we don't have a colour channel (see further)\n", "    plt.figure()\n", "    if activation:\n", "        data = data[:, :, 0]\n", "    \n", "    if cmap is None:\n", "        cmap = plt.get_cmap()\n", "    \n", "    # Ensure that zero is in the middle of the colourmap\n", "    if diverging:\n", "        plt.imshow(data, vmin=vmin, vmax=vmax, cmap=cmap)\n", "    else:\n", "        plt.imshow(data, cmap=cmap)\n", "    plt.axis('off')\n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Get the weights of the first convolutional layer \n", "# (so that's the second layer, just after the input layer)\n", "first_layer_weights = vgg_model.layers[1].get_weights()\n", "# first_layer_weights[0] stores the connection weights of the layer\n", "# first_layer_weights[1] stores the biases of the layer\n", "# For now we're just interested in the connections\n", "filters = first_layer_weights[0]\n", "\n", "# Visualise the filters \n", "# (swapaxes to get it to be in the appropriate ordering of dimensions)\n", "vis_square(np.swapaxes(filters, 0, 3))\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["You can see how each filter detects a different property of the input image. Some are designed to respond to certain colours, while some other -- the greyscale looking ones -- detects changes in brightness such as edges. \n", "\n", "Another way of visualising the network is to see which neurons get activated as the images traverses the network. A neuron outputing a high value means the pattern it has learnt to detect has been observed. \n", "Let's apply this to our kitten image."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def get_layer_output(model, image, layer):\n", "    output_fn = keras.Model(inputs=[model.layers[0].input], outputs=[model.layers[layer].output])\n", "    return output_fn([image])\n", "\n", "# retrieve the activation of the first convolutional layer\n", "layer_output = get_layer_output(vgg_model, img_t, 1)\n", "\n", "# visualise\n", "cmap = plt.get_cmap('bwr')\n", "cmap.set_bad('black', alpha=1.)  # set np.nan values on border to be black\n", "\n", "print(layer_output.shape)\n", "\n", "vis_square(np.swapaxes(layer_output, 0, 3), padsize=20, \n", "           activation=True, diverging=True, normalize=False, cmap=cmap)\n", "plt.colorbar(fraction=.045)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**It's worth spending a moment to understand what is going on here. Do you understand why all these activations are positive?**\n", "\n", "**Each pixel in this image is a different neuron in the neural network. Neurons on the same image sample share the same weights and therefore detect the same feature. You can compare the visualised filters above with their corresponding image sample. What filter helps detect grass?**\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["\n", "Using this method, it is possible to visualise the deeper parts of the neural network, although they become much harder to interpret. You can visualise the output of the second convolutional layer:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["layer_output = get_layer_output(vgg_model, img_t, 3)\n", "vis_square(np.swapaxes(layer_output, 0, 3), padsize=15,\n", "           activation=True, diverging=True, normalize=False, cmap=cmap)\n", "plt.colorbar(fraction=.045)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["And the eighth layer:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["layer_output = get_layer_output(vgg_model, img_t, 9)\n", "vis_square(np.swapaxes(layer_output, 0, 3)[0:64], padsize=5,\n", "           activation=True, diverging=True, normalize=False, cmap=cmap)\n", "plt.colorbar(fraction=.045)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["As we get further down the network, the representations become smaller in their spatial features thanks to the pooling layers. The final convolutional layers only have dimensions 14 by 14."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# visualize the final convolutional layer\n", "layer_output = get_layer_output(vgg_model, img_t, 29)\n", "vis_square(np.swapaxes(layer_output, 0, 3)[0:64], padsize=1,\n", "           activation=True, diverging=True, normalize=False, cmap=cmap)\n", "plt.colorbar(fraction=.045)\n", "plt.show()\n"]}]}
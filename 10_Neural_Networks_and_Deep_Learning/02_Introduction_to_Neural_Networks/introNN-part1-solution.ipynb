{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Introduction to Neural Networks "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## The Perceptron\n", "\n", "\n", "To get an intuitive idea about Neural Networks, we will code an elementary perceptron. In this example we will illustrate some of the concepts you have just seen, build a small perceptron and make a link between Perceptron and linear classifier.\n", "\n", "### Generating some data\n", "\n", "Before working with the MNIST dataset, you'll first test your perceptron implementation on a \"toy\" dataset with just a few data points. This allows you to test your implementations with data you can easily inspect and visualise without getting lost in the complexities of the dataset itself.\n", "\n", "\n", "Start by loading two basic libraries: `matplotlib` and `numpy`"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Load the libraries ...\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Also, tell jupyter to show plots inside the notebook with a magic command\n", "# hint: http://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-matplotlib\n", "%matplotlib inline\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Then let us generate some points in a 2D space that will form our dataset (you can add points later if you'd like)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["crosses = np.array(\n", "    [[0.5, 1.0], [1.0, 1.5], [1.5, 1.5], [2.0, 1.2], [3.0, 1.7], [1.5, 1.1], [2.1, 1.7]]\n", ")\n", "circles = np.array(\n", "    [[3.0, 0.5], [4.0, 1.0], [5.0, 0.7], [4.0, 0.2], [5.1, 0.3], [4.2, 0.7]]\n", ")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Visualising the data\n", "\n", "Using `matploblib`, you can display the crosses as crosses (use `marker='x'`) and the circles as circles (use `marker='o'`). You will need to specify that you don't want a line using `linestyle='none'`. You can observe that the points are very easily separable. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# add your code here to visualise the points\n", "plt.figure()\n", "# you could use plt.scatter in a similar fashion\n", "plt.plot(crosses[:, 0], crosses[:, 1], marker=\"x\", linestyle=\"none\", label=\"cross\")\n", "plt.plot(circles[:, 0], circles[:, 1], marker=\"o\", linestyle=\"none\", label=\"circle\")\n", "plt.legend()\n", "plt.ylim((0, 2))\n", "plt.xlim((0, 6))\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Computing the output of a Perceptron\n", "\n", "\n", "Let us consider the problem of building a classifier that for a given **new** point will return whether it belongs to the crosses (class 1) or circles (class 0). So for example it would take `(2, 1.5)` and return `1`. \n", "\n", "Define a function `out_perceptron` which takes a 2d vector `x`, a 2d weight vector `w` and a bias `b` and returns the output following the step rule:\n", "\n", "$$\n", "\\text{output} = \\left\\{\\begin{align} 1\\,\\, &\\text{if}\\,\\, \\langle x, w\\rangle -b \\, >\\,0 \\\\ 0\\,\\, &\\text{otherwise}\\end{align}\\right.\n", "$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# add your code here...\n", "def out_perceptron(x, w, b):\n", "    innerProd = np.dot(x, w)\n", "    output = 0\n", "    if innerProd > b:\n", "        output = 1\n", "    return output\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["You can then enrich the function so that it can take a **sequence of inputs** (in the form of a matrix where each line of the matrix is one input vector) and return the corresponding **sequence of outputs**. \n", "\n", "One way of doing this is to loop over the rows of `X` and for each of them, use the function `out_perceptron` that you just wrote. Store the results in an array `outputs` and return that. Call that function `multi_out_perceptron`/\n", "\n", "Once you have that, you can try optimising the function by using a matrix-vector product; call the resulting function `multi_out_perceptron_2` (and make sure it leads to the same results!)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# add your code here to implement multi_out_perceptron\n", "def multi_out_perceptron(X, w, b):\n", "    n_instances = X.shape[0]\n", "    outputs = np.zeros(n_instances)\n", "    for i in range(0, n_instances):\n", "        outputs[i] = out_perceptron(X[i, :], w, b)\n", "    return outputs\n", "\n", "\n", "\n", "\n", "# (bonus) add your code here to implement multi_out_perceptron_2\n", "def multi_out_perceptron_2(X, w, b):\n", "    return (np.dot(X, w) > b).astype(float)\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## = checkpoint 1 =\n", "\n", "here, you should copy-paste the following code. If it returns `True` you're good to go on.\n", "\n", "```python\n", "np.random.seed(1234)\n", "X = np.random.randn(10, 5)\n", "w = np.random.randn(5)\n", "b = np.random.randn()\n", "np.all(multi_out_perceptron_2(X, w, b) == np.array([ 1.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.]))\n", "```\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["\n", "np.random.seed(1234)\n", "X = np.random.randn(10, 5)\n", "w = np.random.randn(5)\n", "b = np.random.randn()\n", "np.all(multi_out_perceptron_2(X, w, b) == np.array([ 1.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.]))\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Trying different weights and biases\n", "\n", "You now have a method that can compute the outputs predicted by an **untrained** perceptron. Can you try picking different weights and biases and see how well you can classify the crosses and circles? \n", "\n", "**Note**: to join the crosses and circles into one `instances` matrix, you can use `np.concatenate((crosses, circles), axis=0)`.\n", "\n", "You can maybe start with `w=[1, 1]` and `b=1` and output the result of `multi_out_perceptron`. What is your analysis?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# your code here...\n", "test_w1 = [1.0, 1.0]\n", "test_b1 = 1.0\n", "instances = np.concatenate((crosses, circles), axis=0)\n", "print(multi_out_perceptron(instances, test_w1, test_b1))\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["With the suggested weights and biases (`([1, 1],1)`), you should see something like \n", "\n", "> `[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]` \n", "\n", "which is clearly not great! Now try with `w=[-0.5, 1]` and `b=-0.2`, what do you observe? "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# your code here...\n", "test_w2 = [-0.5, 1.0]\n", "test_b2 = -0.2\n", "print(multi_out_perceptron(instances, test_w2, test_b2))\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### How did we get there?\n", "\n", "This is much better (100% correct on the training data). \n", "To obtain these values, we found a **separating hyperplane** (here a line) between the points. \n", "The equation of the line is \n", "\n", "$ y = 0.5x-0.2 $\n", "\n", "\n", "**Quiz**\n", "- **Can you explain why this line corresponds to the weights and bias we used?**\n", "- **Is this separating line unique? does it matter?**"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Illustrating the output of the Perceptron and the separating line\n", "\n", "Copy-paste your code to visualise the crosses and circles above and overlay the separating line in red. \n", "\n", "Can you modify the parameters of the line a little bit and still find a separating line that \"works\"? "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# your code here..\n", "xx = np.linspace(0, 6)\n", "yy1 = 0.5 * xx - 0.2\n", "yy2 = 0.4 * xx - 0.3\n", "\n", "plt.figure()\n", "plt.plot(xx, yy1, color=\"red\", label=\"$0.5x - 0.2$\")\n", "plt.plot(xx, yy2, color=\"orange\", label=\"$0.4x - 0.3$\")\n", "plt.plot(crosses[:, 0], crosses[:, 1], marker=\"x\", linestyle=\"none\")\n", "plt.plot(circles[:, 0], circles[:, 1], marker=\"o\", linestyle=\"none\")\n", "plt.ylim((0, 2))\n", "plt.xlim((0, 6))\n", "plt.legend()\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Testing a few new points\n", "\n", "Can you add the following `test_points` on the plot and discuss how they would be classified? "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["test_points = np.array([[1, 0.5], [5, 1.5], [3, 1.1]])\n", "test_classes = [1, 0, 1]\n", "symbol_map = {1: \"x\", 0: \"o\"}"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# your code here to visualise the situation\n", "plt.figure()\n", "plt.plot(xx, yy1, color=\"red\", label=\"$0.5x - 0.2$\")\n", "plt.plot(xx, yy2, color=\"orange\", label=\"$0.4x - 0.3$\")\n", "plt.plot(crosses[:, 0], crosses[:, 1], marker=\"x\", linestyle=\"none\")\n", "plt.plot(circles[:, 0], circles[:, 1], marker=\"o\", linestyle=\"none\")\n", "plt.ylim((0, 2))\n", "plt.xlim((0, 6))\n", "plt.legend()\n", "\n", "# the points\n", "for (x, y), s in zip(test_points, test_classes):\n", "    marker = symbol_map[s]\n", "    plt.plot(x, y, marker=marker, color=\"black\", markersize=10)\n", "\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Loss Function\n", "\n", "We've defined our model but how do we pick the right weights? Simply we pick the best weights to fit our training data.\n", "\n", "To measure this \"fit\" we need to define a loss function.\n", "\n", "Implement a loss function called `loss_function(predictions, test_classes)`, where `predictions` are the model outputs from the test_points (`test_classes` is defined above). The function should return a loss value, measuring how well the model predictions match the test_classes. This value should be lower if the predictions are more correct and higher if less correct."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["predictions = multi_out_perceptron(test_points, [-0.5, 1.0], -0.2)\n", "\n", "# def loss_function(predictions, test_classes):\n", "def loss_function(predictions, test_classes):\n", "    # mean squared error\n", "    return np.mean((predictions - test_classes) ** 2)\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## = checkpoint 2 =\n", "\n", "here, you should copy-paste the following code. If it returns `True` you're good to go on.\n", "\n", "```python\n", "predictions_a = multi_out_perceptron(test_points, [-0.5, 1.0], -0.2)\n", "loss_a = loss_function(predictions_a, test_classes)\n", "predictions_b = multi_out_perceptron(test_points, [-0.4, 1.0], -0.3)\n", "loss_b = loss_function(predictions_b, test_classes)\n", "loss_b < loss_a\n", "```\n", "\n", "Don't worry if this is tricky - we will cover loss functions in the next session."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["\n", "predictions_a = multi_out_perceptron(test_points, [-0.5, 1.0], -0.2)\n", "loss_a = loss_function(predictions_a, test_classes)\n", "predictions_b = multi_out_perceptron(test_points, [-0.4, 1.0], -0.3)\n", "loss_b = loss_function(predictions_b, test_classes)\n", "loss_b < loss_a\n", "\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Gradient Descent Demonstration\n", "\n", "We have a model and we have a way of measuring how good it is. Next we need an efficient way to update the model weights to get the best fit, ie to minimize the loss of the model (on some training data).\n", "\n", "We will use gradient descent to do this. We will cover this in the next session. The following implements gradient descent as a demonstration.\n", "\n", "## Considering some function\n", "\n", "Let's consider the following arbitrary function and its gradient:\n", "\n", "$f(x) = \\exp(-\\sin(x))x^2$\n", "\n", "$f'(x) = -x \\exp(-\\sin(x)) (x\\cos(x)-2)$\n", "\n", "It is convenient to define python functions which return the value of the function and its gradient at an arbitrary point $x$;\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def function(x):\n", "    return np.exp(-np.sin(x)) * (x**2)\n", "\n", "\n", "def gradient(x):\n", "    return -x * np.exp(-np.sin(x)) * (x * np.cos(x) - 2)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Visualising the function"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["x = np.linspace(-10, 10, 500)\n", "plt.figure()\n", "plt.plot(x, function(x))\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Implementing a simple GD\n", "\n", "Now let us implement a simple Gradient Descent that uses constant stepsizes. Define two functions:\n", "\n", "1. simplest version which doesn't store the intermediate steps that are taken. \n", "2. a version which does store the steps (useful to visualize what is going on and explain some of the typical behaviour of GD).\n", "\n", "Let's call them `simple_GD` and `simple_GD2`. The parameters of both functions will be the initial point `x0`, the stepsize, and the number of steps to be taken."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def simple_GD(x0, stepsize, nsteps):\n", "    x = x0\n", "    for k in range(0, nsteps):\n", "        x -= stepsize * gradient(x)\n", "    return x\n", "\n", "\n", "def simple_GD2(x0, stepsize, nsteps):\n", "    x = np.zeros(nsteps + 1)\n", "    x[0] = x0\n", "    for k in range(0, nsteps):\n", "        x[k + 1] = x[k] - stepsize * gradient(x[k])\n", "    return x"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Testing different situations\n", "\n", "Try your algorithm `simple_GD` in the following cases:\n", "\n", "* $x_0=1, \\delta=0.1, n=100$\n", "* $x_0=6, \\delta=0.1, n=100$\n", "* $x_0=8, \\delta=0.01, n=100$\n", "\n", "Can you discuss the results you obtained by having a look at the plot of the function? \n", "\n", "### Visualising the cases\n", "\n", "We suggest below a function `viz` which shows the path taken by the gradient descent when computed using `simpleGD2`. \n", "\n", "We the use it in the different cases above in order to see what the Gradient Descent does. Try to interpret the different cases."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def viz(x, a=-10, b=10):\n", "    xx = np.linspace(a, b, 100)\n", "    yy = function(xx)\n", "    ygd = function(x)\n", "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n", "    plt.sca(ax[0])\n", "    plt.plot(xx, yy)\n", "    plt.plot(x, ygd, \".-\", color=\"k\", label=\"steps\", alpha=0.5)\n", "    plt.plot(x[0], ygd[0], marker=\"o\", color=\"green\", markersize=10, label=\"start\")\n", "    plt.plot(\n", "        x[len(x) - 1],\n", "        ygd[len(x) - 1],\n", "        marker=\"o\",\n", "        color=\"red\",\n", "        markersize=10,\n", "        label=\"end\",\n", "    )\n", "    plt.title(\"Global Picure\")\n", "    plt.sca(ax[1])\n", "    plt.title(\"Zoom (N.B. both axes diff scales)\")\n", "    xx = np.linspace(min(x), max(x), 100)\n", "    yy = function(xx)\n", "    plt.plot(xx, yy)\n", "    plt.plot(x, ygd, \".-\", color=\"k\", label=\"steps\", alpha=0.5)\n", "    plt.plot(x[0], ygd[0], marker=\"o\", color=\"green\", markersize=10, label=\"start\")\n", "    plt.plot(\n", "        x[len(x) - 1],\n", "        ygd[len(x) - 1],\n", "        marker=\"o\",\n", "        color=\"red\",\n", "        markersize=10,\n", "        label=\"end\",\n", "    )\n", "    plt.legend()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["x1 = simple_GD2(3, 0.1, 100)\n", "x2 = simple_GD2(6, 0.1, 100)\n", "x3 = simple_GD2(8, 0.01, 100)\n", "x4 = simple_GD2(3, 0.5, 100)\n", "\n", "viz(x1)\n", "plt.show()\n", "viz(x2)\n", "plt.show()\n", "viz(x3)\n", "plt.show()\n", "viz(x4)\n", "plt.show()"]}]}
{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Introduction to Gradient Descent"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["This notebook looks at:\n", "\n", "* Implementing basic gradient descent.\n", "* The effects of step size on gradient descent.\n", "* The effects of starting position on gradient descent."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Import libraries\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Implementing Gradient Descent\n", "\n", "We will return to our mystery function at the end of this notebook, but for now let's deal with a different example.\n", "\n", "Create a function $\\mathcal{L}(\\theta)=\\sqrt{1-\\exp(-\\theta^2/10)}$. Plot it on the interval (-4,4)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# define the function f and plot on the interval (-4, 4)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The gradient of that function is\n", "\n", "$$g(\\theta) = \\nabla \\mathcal{L}(\\theta) = \\theta \\frac{\\exp(-\\theta^2/10)}{10\\mathcal{L}(\\theta)} $$\n", "\n", "(\ud83e\udd89 *Note; $\\nabla$ is often used as a shorthand for $\\frac{\\partial \\mathcal{L}}{\\partial \\theta}$; by writing $\\mathcal{L}(\\theta)$ we make it clear the derivatives are with respect to $\\theta$. For those interested, the derivation of the gradient is provided at the end of this notebook.*)\n", "\n", "Create this function $g(\\theta)$, the gradient of $\\mathcal{L}(\\theta)$, and plot it together with the original function. Make sure you understand the relationship between the slope of $\\mathcal{L}(\\theta)$ and $g(\\theta)$."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# define the function g and plot on the interval (-4, 4) with f\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Write a function `simpleGD` with **inputs**:\n", "* a starting point `theta_0`\n", "* a function `L`\n", "* the function L's gradient `g`\n", "* a learning rate `lmbda` (also known as the stepsize)\n", "* and a number of steps, `nr_steps`, to iterate over\n", "\n", "and that **returns**: \n", "* a 2D array of points:\n", "```\n", "    array([[theta_0, L(theta_0)],\n", "           [theta_1, L(theta_1)],\n", "           ....\n", "           [theta_n, L(theta_n)])\n", "```\n", "\n", "where each `[theta_t, L(theta_t)]` pair is generated from the previous pair using simple gradient descent:\n", "\n", "$$\\theta_{t} = \\theta_{t-1} - \\lambda \\frac{\\partial \\mathcal{L}(f(\\theta_{t-1};x))}{\\partial \\theta}.  $$"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# create the function simpleGD:\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Now run your function for `10` steps with inputs $\\mathcal{L}(\\theta)$ and $g(\\theta)$ starting at `-3` and with $\\lambda$ set to `0.9`"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# apply 10 steps of gradient descent to L (with grad g) starting at -3 with lambda 0.9\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Finally, plot the outputs against the underlying function $\\mathcal{L}(\\theta)$."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# and plot the output points from simpleGD with the required settings\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Choice of learning rate\n", "\n", "To show the importance of a good step size/learning rate, repeat the above procedure with a function $\\mathcal{L}(\\theta) = \\theta^2$. Take 10 steps of gradient descent starting at `-1` and use `lmbda = 1.05`.\n", "\n", "What's the problem here?!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# create a new f and g, apply, and plot the gradient descent\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Choice of starting location\n", "\n", "Finally, let's consider the function we were working with in the slides. That function was\n", "\n", "$$\\mathcal{L}(\\theta) = \\theta^4 - 4\\theta^3 - 2\\theta^2 + 5\\theta + 31. $$\n", "\n", "Its gradient is\n", "\n", "$$g(\\theta) = 4\\theta^3 - 12\\theta^2 - 4\\theta + 5.$$\n", "\n", "Repeat the 10 step procedure above with `theta = 3` and use `lmbda = 0.01`. Plot the results and $\\mathcal{L}(\\theta)$ in the range `[2.5,4.5]`. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# create a new f and g, apply, and plot the gradient descent\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["What do you notice? Compare your results to the gradient descent we ran on the slides, starting at `theta = -1.5`. What is the difference? Plot the function in the range `[-2,5]` to help see what's going on."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Plot the function from [-2,5]\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Feel free to play around with different step sizes/starting locations on this function."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Footnote:\n", "\n", "Below is the derivation of the gradient for the function $\\mathcal{L}(\\theta)$, for those interested.\n", "\n", "$$\n", "\\begin{aligned}\n", " \\nabla \\mathcal{L}(\\theta) &= \\nabla (1 - \\exp{\\frac{-\\theta^2}{10}})^{\\frac{1}{2}} \\\\\n", " &= \\frac{1}{2}(1 - \\exp{\\frac{-\\theta^2}{10}})^{-\\frac{1}{2}} \\nabla (1 -\\exp{\\frac{-\\theta^2}{10}}) \\\\\n", " &= \\frac{1}{2}(1 - \\exp{\\frac{-\\theta^2}{10}})^{-\\frac{1}{2}} \\nabla (1) - \\frac{1}{2}(1 - \\exp{\\frac{-\\theta^2}{10}})^{-\\frac{1}{2}} \\nabla (\\exp{\\frac{-\\theta^2}{10}}) \\\\\n", " &= - \\frac{1}{2}(1 - \\exp{\\frac{-\\theta^2}{10}})^{-\\frac{1}{2}} \\nabla (\\exp{\\frac{-\\theta^2}{10}}) \\\\\n", " &= - \\frac{1}{2}(1 - \\exp{\\frac{-\\theta^2}{10}})^{-\\frac{1}{2}} \\exp{\\frac{-\\theta^2}{10}} \\nabla (\\frac{-\\theta^2}{10}) \\\\\n", " &= -\\frac{1}{2}(1 - \\exp{\\frac{-\\theta^2}{10}})^{-\\frac{1}{2}} \\exp{\\frac{-\\theta^2}{10}} (\\frac{-\\theta}{5}) \\\\\n", " &= \\frac{1}{2}(1 - \\exp{\\frac{-\\theta^2}{10}})^{-\\frac{1}{2}} \\exp{\\frac{-\\theta^2}{10}} (\\frac{\\theta}{5}) \\\\\n", " &= \\theta \\frac{1}{10 (1 - \\exp{\\frac{-\\theta^2}{10}})^{\\frac{1}{2}}} \\exp{\\frac{-\\theta^2}{10}}\\\\\n", " &= \\theta \\frac{\\exp(-\\theta^2/10)}{10\\mathcal{L}(\\theta)}\n", "\\end{aligned}\n", "$$"]}]}
{"cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Discriminative Classifiers - Diagnosing Breast Cancer\n", "\n", "In this practical, we will work with a real dataset of medical data. The features are generated from images of masses taken from breast tissue. The outcome variable is whether the mass is malignant or benign. More information can be found [here](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29).\n", "\n", "We will use a train/test split to explore the impact of the $k$ on performance, looking at the trade-off between bias and variance. We will also look at how the KNN model is sensitive to the values of the features you use."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["First, we load the data into a DataFrame and assign the features to `X` and the `diagnosis` variable to `y`.\n", "\n", "Look at the distribution of benign (`y==0`) and malignant (`y==1`). What do you notice?"]}, {"cell_type": "code", "execution_count": 1, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import pandas as pd\n", "\n", "data = pd.read_csv(\"data/wisconsin_data.csv\")\n", "X = data.drop(\"diagnosis\", axis=1)\n", "y = data[\"diagnosis\"]\n", "\n", "# Your thoughts here...\n", "print(y.value_counts(normalize=True))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Creating a test/train split\n", "\n", "Split the data into train and test datasets. You could do this manually but the `sklearn.model_selection.train_test_split` function can handle it all. It takes in data `X` and `y` and splits it into `X_train`, `X_test`, `y_train` and `y_test`.\n", "\n", "Use this function to split up your data. Make the test set contain around 20% of the data using `test_size=0.2`.\n", "\n", "Set `stratify=y` to ensure the ratio of classes in `y_train`/`y_test` is preserved and check this is the case."]}, {"cell_type": "code", "execution_count": 2, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=144)\n", "\n", "print(y_train.value_counts(normalize=True))\n", "print(y_test.value_counts(normalize=True))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Training an initial model\n", "\n", "Instantiate a `sklearn.neighbors.KNeighborsClassifier` model with default parameters (`k=5`), named `knn`.\n", "\n", "Use the `.fit()` method to train it on `X_train` and `y_train`.\n", "\n", "Use the `.score()` method of the trained model to find its accuracy using the test set `X_test` and `y_test`"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.neighbors import KNeighborsClassifier\n", "\n", "# Your code here...\n", "neighbours = 5\n", "\n", "knn = KNeighborsClassifier(n_neighbors=neighbours)\n", "\n", "knn.fit(X_train, y_train);\n", "\n", "accuracy = knn.score(X_test, y_test)\n", "\n", "print(accuracy)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Pre-processing data for optimal KNN performance\n", "\n", "Because KNN uses the concept of **distance** between points to determine similarity, if the scales of features differ wildly then it can cause issues.\n", "\n", "For example, if one feature is in the range 1-5, but another in the 400 to 290000, then the Euclidean spaces represented by these features are very far apart. Distances between two points based on these features will be extreme.\n", "\n", "The min/max of `X_train` shows this:"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["X_train.describe()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["It is not the raw feature values that matter, but their size relative to each other. Therefore, we can scale all features to be within the same range. This is normally 0 to 1.\n", "\n", "This can be easily done using `sklearn.preprocessing.MinMaxScaler`:\n", "\n"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.preprocessing import MinMaxScaler\n", "\n", "scaler = MinMaxScaler()\n", "\n", "X_train_scaled = scaler.fit_transform(X_train)\n", "X_test_scaled = scaler.transform(X_test)\n", "\n", "print(f\"New min: {X_train_scaled.min():.3f} New max: {X_train_scaled.max():.3f}\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Train and score a new KNN as before, named `knn_scaled`, using the new scaled data."]}, {"cell_type": "code", "execution_count": 6, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "from sklearn.neighbors import KNeighborsClassifier\n", "\n", "# Your code here...\n", "neighbours = 5\n", "\n", "knn_scaled = KNeighborsClassifier(n_neighbors=neighbours)\n", "\n", "knn_scaled.fit(X_train_scaled, y_train);\n", "\n", "accuracy = knn_scaled.score(X_test_scaled, y_test)\n", "\n", "print(f\"Accuracy: {accuracy:.3f}\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Accuracy has improved quite a bit!"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## The impact of `k` on accuracy\n", "\n", "Recall that the value of $k$ in KNN impacts model **bias** (how well the model captures relevant relations in the features) and model **variance** (how sensitive the model is to noise in the features).\n", "\n", "A KNN model is most prone to overfitting when $k$ is low, and underfitting when $k$ is high.\n", "\n", "For values of $k$ in `range(1, 400)`, create a model using that value of $k$ and `.fit()` it using `X_train_scaled` and `y_train`.\n", "\n", "Use the `.score()` method on the train data (`X_train_scaled` and `y_train`) and store the resulting score in `accs_train`.\n", "\n", "Use the `.score()` method on the test data (`X_test_scaled` and `y_test`) and store the resulting score in `accs_test`.\n", "\n", "(This might take 30 seconds or so to complete!)"]}, {"cell_type": "code", "execution_count": 7, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["accs_train = []\n", "accs_test = []\n", "\n", "# Your code here...\n", "for i in range(1, 400):\n", "    knn = KNeighborsClassifier(n_neighbors=i)\n", "\n", "    knn.fit(X_train_scaled, y_train);\n", "\n", "    accs_train.append(knn.score(X_train_scaled, y_train))\n", "    accs_test.append(knn.score(X_test_scaled, y_test))"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The cell below will plot the results for you, of accuracy at various values of $k$. What do you observe?"]}, {"cell_type": "code", "execution_count": 8, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import seaborn as sns\n", "\n", "# Make plot a readable size\n", "sns.set_theme(rc={\"figure.figsize\": (12, 8)})\n", "# Convert data to DataFrame\n", "df = pd.DataFrame(\n", "    zip(accs_train, accs_test, range(1, 400)),\n", "    columns=[\"train data (seen)\", \"test data (unseen)\", \"k\"],\n", ")\n", "# Melt to long format for easy plotting\n", "df = df.melt(var_name=\"Evaluated against:\", id_vars=\"k\", value_name=\"Accuracy\")\n", "# Plot dataframe\n", "g = sns.lineplot(data=df, hue=\"Evaluated against:\", x=\"k\", y=\"Accuracy\")\n", "\n", "\n", "# Your thoughts here...\n", "## Accuracy "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Evaluating KNN: true/false positives/negatives\n", "\n", "The `.score()` method used the accuracy metric - the number of correct classifications out of the total classifications made.\n", "\n", "This doesn't really give the best picture of model performance, though. As you saw when $k>350$, accuracy flat-lines at 0.63. This is because the model is using almost ALL the other data points for classification and around 63% of them are in the benign class.\n", "\n", "A more useful approach is to see how the model performed for each individual class. Especially for health-related tasks, we are interested in:\n", "\n", "* True Positives (TP): Cases in which the tissue is malignant and it was predicted as such.\n", "* True Negatives (TN): Cases in which the tissue is benign (not malignant) and it was predicted as such.\n", "* False Positives (FP): Cases in which the tissue is benign (not malignant) and it was predicted as malignant. (This is often called Type I error.)\n", "* False Negatives (FN): Cases in which the tissue is malignant and it was predicted as benign. (This is often called Type II error.)\n", "\n", "A confusion matrix can show this and can be computed using `pandas.crosstab` then visualised with `seaborn.heatmap`.\n", "\n", "The cell below will do this for you. What do you observe?"]}, {"cell_type": "code", "execution_count": 9, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "\n", "# Make readable size\n", "sns.set_theme(rc={\"figure.figsize\": (18, 6)})\n", "# Get 4 new blank plots in a row\n", "fig, axes = plt.subplots(1, 4)\n", "\n", "# Iterate through a few values of k\n", "for e, k in enumerate([50, 100, 200, 350]):\n", "    # Make model\n", "    knn = KNeighborsClassifier(n_neighbors=k)\n", "    # Train on training data\n", "    knn.fit(X_train_scaled, y_train)\n", "    # Get predictions of test data\n", "    y_pred = knn.predict(X_test_scaled)\n", "\n", "    # Make the confusion matrix. Normalise the cells to show percentages overall\n", "    cm = pd.crosstab(\n", "        y_test, y_pred, rownames=[\"True\"], colnames=[\"Predicted\"], normalize=True\n", "    )\n", "\n", "    # Plot confusion matrix, one on each of the blank axes.\n", "    g = sns.heatmap(\n", "        data=cm, cmap=\"Blues\", square=True, annot=True, ax=axes[e], cbar=False\n", "    )\n", "\n", "    # Label them so it's clear which is which\n", "    g.set_title(f\"k = {k}\")\n", "\n", "\n", "# Your thoughts here...\n", "### K=100 reflects best relation to truth\n", "### K=350 is aweful, it's predicting everything as 0"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Evaluating KNN: precision, recall, F1 score\n", "\n", "True/false positives/negatives can be combined to make new metrics, to give a more concise understanding of how the model is performing.\n", "\n", "* Precision = TP/TP+FP\n", "    * Ratio of correctly predicted positive observations to the total predicted positive observations\n", "* Recall = TP/TP+FN\n", "    * Ratio of correctly predicted positive observations to all of the observations in that class\n", "* F1 Score = 2*(Recall Precision) / (Recall + Precision)\n", "    * Weighted average of Precision and Recall\n", "    \n", "`sklearn.metrics.classification_report` can provide a nice summary of all of these metrics, per class.\n", "\n", "For values of `k` in `[50,100,200,350]`, train and fit a new model on the scaled training data.\n", "\n", "Use the model's `.predict()` method with the scaled test data. Store as `y_pred`.\n", "\n", "Use `classification_report(y_test, y_pred, zero_division=0)` to calculate metrics for the model and print them out.\n", "\n", "(Note: `zero_division=0` will prevent an error from popping up when precision or recall equal 0.)\n", "\n", "What do you observe?"]}, {"cell_type": "code", "execution_count": 10, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from sklearn.metrics import classification_report\n", "\n", "# Your code and thoughts below...\n", "for k in [50,100,200,350]:\n", "    knn = KNeighborsClassifier(n_neighbors=k)\n", "    \n", "    knn.fit(X_train_scaled, y_train)\n", "    \n", "    y_pred = knn.predict(X_test_scaled)\n", "    \n", "    print(f\"Classification for {k} Neighbours:\\n{classification_report(y_test, y_pred, zero_division=0)}\")"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Conclusion\n", "\n", "In this section you trained evaluated a KNN model for diagnosing breast cancer.\n", "\n", "You could also look more into the evaluation metrics for these classifiers. See [the sklearn documentation](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) for a range of classification metrics."]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.9"}}, "nbformat": 4, "nbformat_minor": 4}
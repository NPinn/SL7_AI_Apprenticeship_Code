{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 4, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Introduction to Interpretability and Explainability"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["In this practical, we will be testing ourselves on different concepts we learned about model interpretability and explainability."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Part 1: Motivation for Model Interpretability  (10-15 Mins)\n", "\n", "In this part, we remind ourselves why model interpretability is important. The practical is aimed to refresh some key concepts and important terminology related to model interpretability."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Black box models, Interpretability and Explainability\n", "\n", "For the following questions, type the relevant digit of the correct answer in the code cell provided."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### What is a ***Black-Box*** Model"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["1. A model that monitors everything that is happening in it\n", "2. An unfair model that discriminates against minorities\n", "3. A model that is too complex to understand easily"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["3\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### What is the False statement about model interpretability?"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["1. Usually, interpretability comes at the cost of predictive performance\n", "2. A model is interpretable when it can be directly explained by human experts\n", "3. A Random Forest - a relative of the Decision Tree - is an interpretable model"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["3\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### On the contrary, what is the accurate statement about model explainability?"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["1. Explainable models are self-explanatory although they are very complex\n", "2. Explainable models are also identified as white-box models\n", "3. A Random Forest - a relative of the Decision Tree - is an explainable model"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["3\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Are the following statements `True` or `False`?\n", "\n", "For the following questions, please use the cell box to answer `True` or `False`"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Model Interpretability is important as algorithms can have real world impact"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["True\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### All interpretable models are \"White Box\" models"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["True\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Model explainability has no connection to Ethics and Trust in Computer Systems"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["False\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### No regulatory body considers AI model explainability to be essential at the present"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["False\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Explainability tools can be used by data scientists to debug models"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["True\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Part 2: Model Interpretability at Work  (20-30 mins)\n", "\n", "In this part, we remind ourselves why model interpretability is important. We think about how model interpretability applies to us in our work environment."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### In your work place, what role will you have as a stake holder in demanding explainable machine learning models?\n", "\n", "1. Data Scientist\n", "2. End User\n", "3. Policy Maker"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Identify a mission-critical use-case/application that you can automate using machine learning that would require the resultant model to be interpretable or explainable?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["\"\"\"Eg: shortlisting the number of job applications we get in order to identify the candidates that \n", "should be called for interviews.\"\"\"\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Why do you believe an element of transparency should be a key feature of this model?\n", "\n", "1. For Regulatory Compliance\n", "2. To Enforce Accountability and Control \n", "3. To build Trust and Ethical Soundness\n", "4. Answers 1 and 2\n", "5. Answers 1 and 3\n", "6. Answers 2 and 3\n", "7. All of the Above"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["7\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### What type of data will serve as input to your model?\n", "\n", "1. Tabular Data\n", "2. Text Data\n", "3. Image Data\n", "4. Audio Signals"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["2\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Is extraordinary predictive performance very important for this application to succeed (`True` or `False`)? "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["False # in the context of shortlising candidates, having a few false positives is acceptable. \n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Based on your assessment, what machine learning model will you chose?"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### The model you chose is: \n", "1. An Interpretable Model\n", "2. An Explainable Model"]}]}
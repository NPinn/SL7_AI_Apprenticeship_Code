{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Permutation Feature Importance Practical"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["In this practical we look at Permutation Feature Importance for a dataset that contains both numerical and categorical variables. We investigate what happens when we add random features to the data and (bonus part) look at how adding a correlated feature impacts the permutation feature importance scores.\n", "\n", "First we'll need a few imports:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "from sklearn.metrics import balanced_accuracy_score\n", "from lightgbm.sklearn import LGBMClassifier\n", "from sklearn.impute import SimpleImputer\n", "from sklearn.inspection import permutation_importance\n", "from sklearn.compose import ColumnTransformer\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.preprocessing import OneHotEncoder"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Introduction to Practical"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### The Dataset"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The data (sourced [here](https://archive.ics.uci.edu/ml/datasets/bank+marketing)) is from marketing campaigns of a Portuguese bank. The features in the data are:"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["| Feature     | Description                                                                                                                                                              | Data type   |\n", "|-------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------|\n", "| Age         | Age                                                                                                                                                                      | Numeric     |\n", "| Job         | Type Of Job Out Of   ('Admin.','Blue-Collar','Entrepreneur','Housemaid','Management','Retired','Self-Employed','Services','Student','Technician','Unemployed','Unknown') | Categorical |\n", "| Marital     | Marital Status ('Divorced','Married','Single','Unknown'; Note: 'Divorced'   Means Divorced Or Widowed)                                                                   | Categorical |\n", "| Education   | Education Type   ('Basic.4Y','Basic.6Y','Basic.9Y','High.School','Illiterate','Professional.Course','University.Degree','Unknown')                                       | Categorical |\n", "| Default     | Has Credit In Default? ('No','Yes','Unknown')                                                                                                                            | Categorical |\n", "| Housing     | Has Housing Loan? ('No','Yes','Unknown')                                                                                                                                 | Categorical |\n", "| Loan        | Has Personal Loan? ('No','Yes','Unknown')                                                                                                                                | Categorical |\n", "| Contact     | Contact Communication Type ('Cellular','Telephone')                                                                                                                      | Categorical |\n", "| Month       | Last Contact Month Of Year ('Jan', 'Feb', 'Mar', ..., 'Nov', 'Dec')                                                                                                      | Categorical |\n", "| Day of Week | Last Contact Day Of The Week ( 'Mon','Tue','Wed','Thu','Fri')                                                                                                            | Categorical |\n", "| Campaign    | Number Of Contacts Performed During This Campaign And For This Client                                                                                                    | Numeric     |\n", "| pdays       | Number Of Days That Passed By After The Client Was Last Contacted From A   Previous Campaign (999 Means Client Was Not Previously Contacted)                             | Numeric     |\n", "| Previous    | Number Of Contacts Performed Before This Campaign And For This Client                                                                                                    | Numeric     |\n", "| poutcome    | Outcome Of The Previous Marketing Campaign   ('Failure','Nonexistent','Success')                                                                                         | Numeric     |\n", "| y           | Has The Client Subscribed A Term Deposit? ('Yes','No')                                                                                                                   | Binary      |"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Functions"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["A function has been pre-written to train a LightGBM model for data with a mixture of numeric and categorical features. It takes as inputs: `X_train`, `X_test`, `y_train`, `y_test`, a list of the names of the numerical features `numerical_features` and a list of the names of the categorical features `categorical_features`. It outputs a trained LightGBM classifier `lgbm`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def train_lgbm(X_train, X_test, y_train, y_test, numerical_features, categorical_features):\n", "    \n", "    categorical_encoder = OneHotEncoder(handle_unknown='ignore')\n", "    \n", "    numerical_pipe = Pipeline([\n", "        ('imputer', SimpleImputer(strategy='mean'))\n", "    ])\n", "\n", "    preprocessing = ColumnTransformer(\n", "        [('cat', categorical_encoder, categorical_features),\n", "         ('num', numerical_pipe, numerical_features)])\n", "\n", "    lgbm = Pipeline([\n", "        ('preprocess', preprocessing),\n", "        ('classifier', LGBMClassifier(class_weight=\"balanced\", n_jobs=-1))\n", "    ])\n", "\n", "    lgbm.fit(X_train, y_train)\n", "    \n", "    print(\"LightGBM train accuracy: %0.3f\" % lgbm.score(X_train, y_train))\n", "    print(\"LightGBM test accuracy: %0.3f\" % lgbm.score(X_test, y_test))\n", "    print(\"LightGBM balanced test accuracy: %0.3f\" % balanced_accuracy_score(y_test, lgbm.predict(X_test)))\n", "    \n", "    return lgbm"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Another function has been pre-written to generate the permutation feature importance scores and plot. The function takes a trained model `model`, train or test set features `X` and corresponding labels `y`. It produces a visualisation of the box-plots of feature importance scores for that model and data. Feel free to play around with the parameters `n_repeats` and `random_state`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def plot_pfi(model, X, y):\n", "\n", "    result = permutation_importance(model, X, y, n_repeats=10,\n", "                                    random_state=42, n_jobs=2, scoring=\"balanced_accuracy\")\n", "    sorted_idx = result.importances_mean.argsort()\n", "\n", "    fig, ax = plt.subplots()\n", "    ax.boxplot(result.importances[sorted_idx].T,\n", "               vert=False, labels=X.columns[sorted_idx])\n", "    ax.set_title(\"Permutation Feature Importances\")\n", "    fig.tight_layout()\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Exercises"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Part 1: Load Data, Train LightGBM Model and Plot Permutation Feature Importances"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Exercise 1:** use the `pd.read_csv()` function to load in the file `data/bank.csv`. Call the dataframe `df`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["df = pd.read_csv(\"data/bank.csv\")\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Exercise 2:** use the `head()` function to view the first few samples in the dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["df.head()\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Exercise 3:** split the data into labels `y` and features `X`. Hint: use the `.drop(\"y\", axis=1)` function to extract a dataframe without the labels."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["y = df[\"y\"]\n", "X = df.drop(\"y\", axis=1)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Exercise 4:** run the command `y.map({\"no\": 0, \"yes\": 1})` to map the labels from \"no\" to 0 and \"yes\" to 1. This makes it easier to train our model."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["y = y.map({\"no\": 0, \"yes\": 1})\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Exercise 5:** define two lists `numerical_features` and `categorical_features` which contain the names of the numerical and categorical features respectively. Hint: use `X.dtypes` to look at the datatypes (object is a categorical variable). We have to treat these types of variables separately when training the model."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["X.dtypes\n", "numerical_features = [\"age\", \"campaign\", \"pdays\", \"previous\"]\n", "categorical_features = [\"job\", \"marital\", \"education\",\"default\", \"housing\", \"loan\", \n", "                        \"contact\", \"month\", \"day_of_week\", \"poutcome\"]\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Exercise 6:** use the `train_test_split()` function to split the data into `X_train`, `X_test`, `y_train`, `y_test` to prepare for training the model."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(X, y)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Exercise 7:** use the `train_lgbm()` function defined above to train and observe the performance of a LGBM classifier on our data. Remember to pass in the correct arguments (see the function in the Introduction to Practical section above). Because there is a class imbalance in our data, the function also prints the balanced test accuracy."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["lgbm = train_lgbm(X_train, X_test, y_train, y_test, numerical_features, categorical_features)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Exercise 8:** use the `plot_pfi()` function defined above to plot the permutation feature importance scores on the *training* set. Hint: look above to see which arguments you need to pass in."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["plot_pfi(lgbm, X_train, y_train)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Exercise 9:** repeat exercise 8 on the *test* set and observe differences between the two plots."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["plot_pfi(lgbm, X_test, y_test)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Note that the permutation feature importance can be negative for features on the test set, but not the training set. Why is this the case?"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Part 2: Add Random Features\n", "\n", "The following piece of code loads in the data (as you did in Exercise 1, 3 and 4). It then defines two new features `random_cat` and `random_num` which are completely random categorical and numerical variables respectively. This part of the practical looks at how adding random features (which should not be informative to the model) can affect permutation feature importance.\n", "\n", "Run the following cell to define a new dataset with the `random_cat` and `random_num` features added, and train a new model `lgbm_rand` on this data."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["df_rand = pd.read_csv(\"data/bank.csv\")\n", "y_rand = df_rand[\"y\"]\n", "X_rand = df_rand.drop(\"y\", axis=1)\n", "y_rand = y_rand.map({\"no\": 0, \"yes\": 1})\n", "\n", "rng = np.random.RandomState(seed=42)\n", "X_rand[\"random_cat\"] = rng.randint(3, size=X_rand.shape[0])\n", "X_rand[\"random_num\"] = rng.randn(X_rand.shape[0])\n", "\n", "numerical_features_rand = [\"age\", \"campaign\", \"pdays\", \"previous\", \"random_num\"]\n", "categorical_features_rand = [\"job\", \"marital\", \"education\",\"default\", \"housing\", \"loan\",\n", "            \"contact\", \"month\", \"day_of_week\", \"poutcome\", \"random_cat\"]\n", "    \n", "X_train_rand, X_test_rand, y_train_rand, y_test_rand = train_test_split(X_rand, y_rand)\n", "\n", "lgbm_rand = train_lgbm(X_train_rand, X_test_rand, y_train_rand, y_test_rand, numerical_features_rand, categorical_features_rand)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Exercise 10:** use the `plot_pfi()` function to plot the permutation feature importance of the `lgbm_rand` model on the *training* data `X_train_rand` and `y_train_rand`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["plot_pfi(lgbm_rand, X_train_rand, y_train_rand)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Exercise 11:** use the `plot_pfi()` function to plot the permutation feature importance of the `lgbm_rand` model on the *test* data `X_test_rand` and `y_test_rand`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["plot_pfi(lgbm_rand, X_test_rand, y_test_rand)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Interpretation of results:** \n", "\n", "Note that both random features have very low importances (close to 0) as expected on the test set in Exercise 11 - this is good news!\n", "\n", "However, looking at the training set plot in Exercise 10 reveals that `random_num` gets a significantly higher importance ranking than when computed on the test set. The difference between those two plots is a confirmation that the model has enough capacity to use that random numerical feature to overfit: it has found information in the random numbers that we know it shouldn't have."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Bonus Part 3: Add a Correlated Feature (Season)\n", "\n", "The following piece of code loads in the data (as you did in Exercise 1, 3 and 4). It then defines a new feature `season` which is defined using the `month` feature. Clearly `season` and `month` are highly correlated, so this part of the practical looks at how adding correlated features can affect permutation feature importance.\n", "\n", "Run the following cell to define a new dataset with the `season` feature, and train a new model `lgbm_corr` on this data."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["df_corr = pd.read_csv(\"data/bank.csv\")\n", "y_corr = df_corr[\"y\"]\n", "X_corr = df_corr.drop(\"y\", axis=1)\n", "y_corr = y_corr.map({\"no\": 0, \"yes\": 1})\n", "\n", "for month in [\"dec\", \"jan\", \"feb\"]:\n", "    X_corr.loc[X_corr[\"month\"] == month, \"season\"] = \"winter\"\n", "for month in [\"mar\", \"apr\", \"may\"]:\n", "    X_corr.loc[X_corr[\"month\"] == month, \"season\"] = \"spring\"\n", "for month in [\"jun\", \"jul\", \"aug\"]:\n", "    X_corr.loc[X_corr[\"month\"] == month, \"season\"] = \"summer\"\n", "for month in [\"sep\", \"oct\", \"nov\"]:\n", "    X_corr.loc[X_corr[\"month\"] == month, \"season\"]= \"autumn\"\n", "\n", "numerical_features_corr = [\"age\", \"campaign\", \"pdays\", \"previous\"]\n", "categorical_features_corr = [\"job\", \"marital\", \"education\",\"default\", \"housing\", \"loan\",\n", "            \"contact\", \"month\", \"day_of_week\", \"poutcome\", \"season\"]\n", "    \n", "X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split(X_corr, y_corr)\n", "\n", "lgbm_corr = train_lgbm(X_train_corr, X_test_corr, y_train_corr, y_test_corr, numerical_features_corr, categorical_features_corr)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Exercise 12:** use the `plot_pfi()` function to plot the permutation feature importance of the `lgbm_corr` model on the *training* data with the `season` variable."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["plot_pfi(lgbm_corr, X_train_corr, y_train_corr)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Exercise 13:** use the `plot_pfi()` function to plot the permutation feature importance of the `lgbm_corr` model on the *test* data with the `season` variable."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["plot_pfi(lgbm_corr, X_test_corr, y_test_corr)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We have to be slightly careful directly comparing these results to the original model in Part 1, as they have of course used different models altogether! But comparing the training plot to the training plot produced in Part 1, we see the importance score for `month` decreases slightly when `season` is added in. Comparing the training and test plots in this part, we see that `season` has a larger impact in the results on the test set than it did for training. To see more about Permutation Feature Importance with correlated variables, see [this](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py) tutorial."]}]}
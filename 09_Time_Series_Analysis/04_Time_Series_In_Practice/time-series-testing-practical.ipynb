{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Google Trends Data\n", "\n", "Google trends is a very useful source of time series data. Google collects information on the popularity of searches over time and we can use this to get an idea of regional or global interest in any topic. \n", "\n", "Note: You can also add comparisons if you are interested in multivariate time series analysis. \n", "\n", "### Interest in Climate Change \n", "\n", "Here we consider the searches for 'climate change'. By considering global Google searches, we can see if interest in climate change has increased over the last 5 years, and forecast if this interest will increase or not. \n", "\n", "You can see the source of this data here: https://trends.google.com/trends/explore?date=today%205-y&q=climate%20change\n", "\n", "In particular here we will explore using AR and ARMA models for modelling this Google trends data, specifically searches of 'climate change' over the last 5 years worlwide. We will then explore methods for testing the model fits and the forecasts."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import re\n", "import pandas as pd\n", "from datetime import datetime\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "from statsmodels.api import tsa\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.metrics import mean_absolute_error\n", "from statsmodels.tsa.ar_model import ar_select_order\n", "from statsmodels.graphics.tsaplots import plot_acf\n", "\n", "%matplotlib inline"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## 1) Load and transform the data\n", "\n", "Load the data into a `pandas.DataFrame` object. You can use the `pd.read_csv` function with the parameter `skiprows` to ignore the first row. \n", "\n", "Then:\n", "\n", "1) Make the `Week` column of datetime format\n", "\n", "2) Set the `Week` as the index column\n", "\n", "3) Convert the dataframe to a pandas series by selecting the one remaining column `climate change: (Worldwide)`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# For our model building later, we attach a data frequency to the times. This is not essential but avoids warnings\n", "df.index = pd.DatetimeIndex(\n", "    df.index.values,\n", "    freq=df.index.inferred_freq\n", ")\n", "\n", "time_series = df"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## 2) Visualise the data with a plot"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## 3) Fit the data\n", "\n", "### Task:\n", "\n", "1) Using `ar_select_order()` select the optimal AR parameter according to the AIC. \n", "\n", "2) Then fit the time series model using `tsa.AutoReg()` and the optimal lag found\n", "\n", "3) Form a prediction starting at the optimal lag value. Hint: use the parameter `start`.\n", "\n", "4) Plot the resulting prediction and original time series"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Residual diagnostics\n", "\n", "By using `time_series[optlag:].value.flatten()` as the original time series (this should align with your prediction), define the residuals from the AR model fit. \n", "\n", "Plot these residuals and the ACF in order to assess if this model is adequte. \n", "\n", "**Recall:** We want the residuals to have zero mean and be uncorrelated"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## 4) Forecasting\n", "\n", "**Note**: forecasting with time series data is **tricky** and usually basic methods do not really provide very good results (especially on realistic data). ARMA models are nice because they are simple but do not expect fantastic performances. (On the other hand, predicting the future is hard! -- who would have thought).\n", "\n", "* Separate the time series into a training set and a test set formed of the last 40 points. \n", "  - To do this you can index `time_series` with `[:-40]` and `[-40:]`\n", "* Fit an AR model on the training data and try to find the optimal lag using the `BIC` criterion, an alternative to the AIC which also accounts for the sample size. \n", "  - Use the `ar_select_order` function on your training data and use the parameters `maxlag` and `ic`\n", "  - Then use the `AutoReg` function with your training data and use the `lags` parameter to specify the optimal lags found\n", "* Predict and show the prediction on the original time series. Did it do a good job? \n", "  - For this step you will find the `.fit()` and `.predict()` functions useful\n", "  - Note that for the prediction, we want to store the indices `[-len(test):]` as these are form our forecast\n", "* compute the MAE\n", "  - The function `mean_absolute_error` will let you do this"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Repeat for an ARMA process, using `arma_order_select_ic` to find the optimal parameters. How does the MAE compare to the AR process?\n", "\n", "Note: in order to run an ARMA model using `statsmodels`, you can use `tsa.ARIMA` and set the `d` in the order as 0."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## 5) Cross-Validation for Time Series Forecasting\n", "\n", "Recall that it is important not to use future observations during a forecast. Below we will define a function to take a certain number of observations (ordered chronologically) as training data and use the remainder as testing data. We will also visualize their predictions :) \n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def Forecasting_crossValidation( time_series, training_size, optlag ):\n", "    \"\"\"\n", "    Given a pd.Series we train a autoregressive model. \n", "    The number of training observation is specified by the variable training_size\n", "    \"\"\"\n", "    train = time_series[:training_size]\n", "    test = time_series[training_size:]\n", "    ar = tsa.AutoReg(train, lags=optlag)\n", "    ar_result = ar.fit()\n", "    prediction = ar_result.predict(end=time_series.index[-1])[-len(test):]\n", "    # You could also use:\n", "    #prediction = ar_result.predict(end=len(time_series)-1)[-len(test):]\n", "    \n", "    # compute the MAE:\n", "    mae = mean_absolute_error(time_series.values[training_size:], prediction)\n", "    print('Mean absolute error: ' + str(mae))\n", "    \n", "    # plot results as well:\n", "    plt.plot(time_series.values, '-o', label='true')\n", "    plt.plot(range(training_size, len(time_series)), prediction, \n", "         '-o', label='out of sample prediction')\n", "    plt.legend();\n", "    \n", "    return prediction\n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["Forecasting_crossValidation( time_series, int(len(time_series)*0.85), 3)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Excercise:\n", "Compute the average mean absolute error (MAE) for all possible forecasts (using a minimum of 30 training points). \n", "\n", "Hint: \n", "\n", "1) Copy the `Forecasting_crossValidation` function and adapt it so that it:\n", "  - has an input parameter allowing you to specify the lags to use for the AR model\n", "  - returns the MAE\n", "  - Name this function `Forecasting_CV`\n", "  Note: consider if you need it to output plots or not\n", "  \n", "2) Loop through the training sizes from length `30` to `len(time_series)-30`. In each loop do the following:\n", "  - Select the optimal AR parameter according to the AIC using `ar_select_order` and just the training data\n", "  - Use the function `Forecasting_CV` with your time series, current training size and selected lag to get a MAE\n", "  - Append this MAE to a list which stores all of the errors from each training size\n", "\n", "3) Print out the mean of your MAE list to get the average MAE for all possible forecasts\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}]}
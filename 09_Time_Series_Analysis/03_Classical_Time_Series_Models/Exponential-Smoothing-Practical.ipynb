{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 5, "cells": [{"cell_type": "markdown", "id": "2aab443d", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Exponential Smoothing Practical\n", "\n", "In this practical we will explore a dataset contianing a company's revenue over time and build forecasts using exponential smoothing\n"]}, {"cell_type": "markdown", "id": "7b329390", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Exercise 1: \n", "\n", "1. Read in `data/MonthlySales.csv`\n", "2. Divide each entry in the revenue column by 1e6 in order to convert to a millions scale\n", "3. Select just the `Period` and `Revenue` columns and save this to a dataframe called `df`"]}, {"cell_type": "code", "execution_count": null, "id": "6b52e1ed", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import pandas as pd\n", "import matplotlib.pyplot as plt\n", "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing\n", "from statsmodels.graphics.tsaplots import plot_acf"]}, {"cell_type": "code", "execution_count": null, "id": "af41722f", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# 1. Import the data\n", "# Your code here\n", "\n", "# Here we have monthly sales data for a company.\n", "# We will focus solely on the revenue and explore if we can model the company revenue using time series.\n", "\n", "# 2. Convert the revenue to be on a scale of millions for readability.\n", "# Your code here\n", "\n", "# 3. Select a subsection\n", "# Your code here\n"]}, {"cell_type": "markdown", "id": "d7cdc59f", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Run the below cells to dropnans and format the datetimes"]}, {"cell_type": "code", "execution_count": null, "id": "ac2d8c3b", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Drop any NaN values\n", "df.dropna(inplace=True)\n", "\n", "# Convert the data to have datetime indices\n", "# df.loc[:,'Period'] = pd.to_datetime(df['Period'], format=\"%d.%m.%Y\")\n", "df[\"Period\"] = pd.to_datetime(df[\"Period\"], format=\"%d.%m.%Y\")\n", "df.set_index(\"Period\", inplace=True)\n", "\n", "# For our model building later, we attach a data frequency to the times. This is not essential but avoids warnings\n", "df.index = pd.DatetimeIndex(df.index.values, freq=df.index.inferred_freq)\n", "df.head()"]}, {"cell_type": "markdown", "id": "bb81083c", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Exercise 2:\n", "\n", "Plot the data and think about features that you see."]}, {"cell_type": "code", "execution_count": null, "id": "246e753e", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here\n"]}, {"cell_type": "markdown", "id": "b910118b", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Exercise 3: \n", "\n", "Split the data into training and test sets, using the last 12 data points as the test set, representing 1 year."]}, {"cell_type": "code", "execution_count": null, "id": "935c5768", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here\n", "\n", "plt.figure()\n", "plt.plot(df)\n", "plt.plot(test)\n", "plt.legend([\"time series\", \"test data\"])"]}, {"cell_type": "markdown", "id": "89e5c0e2", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Exercise 4: \n", "\n", "1. Using the function `SimpleExpSmoothing` with `initialization_method=\"estimated\"` to fit the SES model to the training data. Note that when using .fit() with no provided parameters, the function will run an automatic optimization to select the parameter $\\alpha$ for us.\n", "\n", "2. Then use your model to forecast `len(test)` data points, which is our prediction of the test data, and save the output to `ypred`. The remaining code will plot the result. "]}, {"cell_type": "code", "execution_count": null, "id": "342dfbdd", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here\n", "\n", "plt.figure()\n", "plt.plot(df)\n", "plt.plot(test.index, y_pred)\n", "plt.plot(test)\n", "plt.legend([\"true data\", \"prediction\", \"test data\"])"]}, {"cell_type": "markdown", "id": "260a0271", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["As expected here, we see that the SES model produces a flat forecast based on a weighted average of past values. However, we clearly have a trend component in our data, so let's see if we can improve the forecast. \n", "\n", "Note that you could manually find the trend, difference the data to remove the trend and create a forecast this way, however Holt's Linear Model will essentially do this for us!"]}, {"cell_type": "markdown", "id": "9d9667ba", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Exercise 5:\n", "\n", "Now use the `ExponentialSmoothing` function to create an instance of Holt's linear model (DES). Recall that we can include an additive trend component using `trend=\"add\"`.\n", "\n", "As before, create a forecast of length equal to the test data and call this `y_pred`."]}, {"cell_type": "code", "execution_count": null, "id": "b8a7e484", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here\n", "\n", "plt.figure()\n", "plt.plot(df)\n", "plt.plot(test.index, y_pred)\n", "plt.plot(test)\n", "plt.legend([\"true data\", \"prediction\", \"test data\"])"]}, {"cell_type": "markdown", "id": "34e493f1", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Exercise 6: \n", "\n", "An extension of Holt's Linear Model to include seasonality is known as Holt Winters. This works in a very similar way, adding in a seasonal component to the forecast equation, and creating a seasonal equation which shows a weighted average between the current seasonal index, and the seasonal index of the same season at the last periodicity. \n", "\n", "This is very simply included in the `ExponentialSmoothing` function by including 2 parameters: `seasonal` and `seasonal_periods`. In order to know what period to use, let's first use the autocorrelation function (ACF). \n", "\n", "Using `plot_acf`, explore the ACF for our data, using lags up to 15. "]}, {"cell_type": "code", "execution_count": null, "id": "d15c38a9", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here\n"]}, {"cell_type": "markdown", "id": "3c3cfc9e", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Exercise 7:\n", "\n", "Create a Holt Winters forecast using the following steps:\n", "\n", "1. Use the `ExponentialSmoothing` function with the training data `train`.\n", "2. Specify `seasonal=\"add\"` and `initialization_method=\"estimated\"`\n", "3. Use the largest significant lag as the value for `seasonal_periods` \n", "4. Use `.fit()` to fit the model\n", "5. Using `.forecast(len(test))` create a forecast and store it in `y_pred` "]}, {"cell_type": "code", "execution_count": null, "id": "3b0475f3", "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here\n", "\n", "plt.figure()\n", "plt.plot(df)\n", "plt.plot(test.index, y_pred)\n", "plt.plot(test)\n", "plt.legend([\"true data\", \"prediction\", \"test data\"])"]}, {"cell_type": "markdown", "id": "f304f427", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Note that we could explore the residuals and build out a model for those if we believed there was signal in them. Our forecast would then be the sum of these models.  "]}]}